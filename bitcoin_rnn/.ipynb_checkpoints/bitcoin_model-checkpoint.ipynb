{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import datetime\n",
    "\n",
    "import cufflinks as cf\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "- date : 날짜\n",
    "- sign : 거래량 증감량\n",
    "- amount : 총 거래량\n",
    "- start_price : 시가\n",
    "- end_price : 종가\n",
    "- max_price : 최고가\n",
    "- avg_price : 평균가\n",
    "- min_price : 최소가\n",
    "- count : 총 거래횟수\n",
    "- total_price : 총 거래금액\n",
    "- market_cap : 시총 증감액\n",
    "- 'n'd_end_moving : n일 이동 평균선\n",
    "- upper_band, lower_band : 볼린져 밴드(20,2)\n",
    "- rsi : rsi14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Loading\n",
    "name = {\"Unnamed: 0\":\"date\", \"0\":\"sign\", \"1\":\"amount\", \"2\":\"start_price\", \"3\":\"end_price\", \"4\":\"max_price\", \"5\":\"avg_price\", \"6\":\"min_price\", \"7\":\"count\", \"8\":\"total_price\", \"9\":\"market_cap\"}\n",
    "df_4h = pd.read_csv(\"./data/bitcoin_4h.csv\").rename(columns = name)\n",
    "df_6h = pd.read_csv(\"./data/bitcoin_6h.csv\").rename(columns = name)\n",
    "df_12h = pd.read_csv(\"./data/bitcoin_12h.csv\").rename(columns = name)\n",
    "df_1d = pd.read_csv(\"./data/bitcoin_1d.csv\").rename(columns = name)\n",
    "df_7d = pd.read_csv(\"./data/bitcoin_7d.csv\").rename(columns = name)\n",
    "df_1m = pd.read_csv(\"./data/bitcoin_1m.csv\").rename(columns = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data processing\n",
    "def mapper(x) :\n",
    "    date, time = x.split()\n",
    "    year, month, day = date.split(\"-\")\n",
    "    \n",
    "    result = year +\"-\" + month + \"-01 00:00:00\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4h[\"date\"] = df_4h[\"date\"].map(lambda x : datetime.datetime.strptime(x,'%Y-%m-%d %H:%M:%S'))\n",
    "df_6h[\"date\"] = df_6h[\"date\"].map(lambda x : datetime.datetime.strptime(x,'%Y-%m-%d %H:%M:%S'))\n",
    "df_12h[\"date\"] = df_12h[\"date\"].map(lambda x : datetime.datetime.strptime(x,'%Y-%m-%d %H:%M:%S'))\n",
    "df_1d[\"date\"] = df_1d[\"date\"].map(lambda x : datetime.datetime.strptime(x,'%Y-%m-%d %H:%M:%S'))\n",
    "df_7d[\"date\"] = df_7d[\"date\"].map(lambda x : datetime.datetime.strptime(x,'%Y-%m-%d %H:%M:%S'))\n",
    "df_1m[\"date\"] = df_1m[\"date\"].map(lambda x : datetime.datetime.strptime(mapper(x),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4h = df_4h.set_index(\"date\").sort_index()\n",
    "df_6h = df_6h.set_index(\"date\").sort_index()\n",
    "df_12h = df_12h.set_index(\"date\").sort_index()\n",
    "df_1d = df_1d.set_index(\"date\").sort_index()\n",
    "df_7d = df_7d.set_index(\"date\").sort_index()\n",
    "df_1m = df_1m.set_index(\"date\").sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_moving(df, day, index, v_index) :\n",
    "    new_df = df.copy()\n",
    "    \n",
    "    for idx in range(day-1) :\n",
    "        new_df.iloc[idx, index] = new_df.iloc[idx, v_index]\n",
    "    \n",
    "    for idx in range(day-1, len(new_df)) :\n",
    "        new_df.iloc[idx, index] = sum(list(new_df.iloc[idx-day+1:idx+1,3]))/day\n",
    "        \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_moving(df,index) :\n",
    "    new_df = df.copy()\n",
    "    \n",
    "    for idx in range(len(new_df)) :\n",
    "        new_df.iloc[idx, index] = sum(list(new_df.iloc[:idx+1,3]))/(idx+1)\n",
    "        \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def band(df, lower_index, higher_index) :\n",
    "    new_df = df.copy()\n",
    "    \n",
    "    mean = new_df.iloc[:,3].rolling(window=20,min_periods=0,center=False).mean()\n",
    "    std = new_df.iloc[:,3].rolling(window=20,min_periods=0,center=False).std()\n",
    "    \n",
    "    for idx in range(len(new_df)) :\n",
    "        if idx == 0 :\n",
    "            std[idx] = 10\n",
    "        \n",
    "        new_df.iloc[idx,lower_index] = mean[idx] - 2 * std[idx]\n",
    "        new_df.iloc[idx,higher_index] = mean[idx] + 2 * std[idx]\n",
    "        \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsi(df, index, lst = [1005, 997.703, 994.602, 957, 1044.407, 1050, 1006.549, 1014.364, 1019.493, 1018.24, 993.416, 980.749, 962.89]) :\n",
    "    new_df = df.copy()\n",
    "    \n",
    "    price = new_df[\"end_price\"].values\n",
    "    for p in lst :\n",
    "        price = np.insert(price, 0, p)\n",
    "    delta = np.diff(price)\n",
    "    \n",
    "    for idx in range(13, len(new_df)+13) :\n",
    "        seed = delta[idx-13:idx+1]\n",
    "        up = seed[seed>=0].sum()/14\n",
    "        down = -seed[seed<0].sum()/14\n",
    "        rs = up/down\n",
    "        rsi = 100 - 100 * (1/(1+rs))\n",
    "                           \n",
    "        new_df.iloc[idx-13, index] = rsi\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1d[\"5d_end_moving\"] = 0\n",
    "df_1d[\"10d_end_moving\"] = 0\n",
    "df_1d[\"20d_end_moving\"] = 0\n",
    "df_1d[\"60d_end_moving\"] = 0 \n",
    "df_1d[\"120d_end_moving\"] = 0\n",
    "df_1d[\"180d_end_moving\"] = 0\n",
    "df_1d[\"240d_end_moving\"] = 0\n",
    "df_1d[\"lower_band\"] = 0\n",
    "df_1d[\"higher_band\"] = 0\n",
    "df_1d[\"rsi\"] = 0\n",
    "df_1d[\"all_end_moving\"] = 0\n",
    "\n",
    "df_1d.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_1d = all_moving(df_1d, 20)\n",
    "df_1d = end_moving(df_1d, 5, 10, 20)\n",
    "df_1d = end_moving(df_1d, 10, 11, 20)\n",
    "df_1d = end_moving(df_1d, 20, 12, 20)\n",
    "df_1d = end_moving(df_1d, 60, 13, 20)\n",
    "df_1d = end_moving(df_1d, 120, 14, 20)\n",
    "df_1d = end_moving(df_1d, 180, 15, 20)\n",
    "df_1d = end_moving(df_1d, 240, 16, 20)\n",
    "df_1d = band(df_1d, 17, 18)\n",
    "df_1d = rsi(df_1d, 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4h[[\"start_price\", \"end_price\", \"max_price\", \"avg_price\", \"min_price\"]].iplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_6h[[\"start_price\", \"end_price\", \"max_price\", \"avg_price\", \"min_price\"]].iplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_12h[[\"start_price\", \"end_price\", \"max_price\", \"avg_price\", \"min_price\"]].iplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1d[[\"start_price\", \"end_price\", \"max_price\", \"avg_price\", \"min_price\", \"5d_end_moving\", \"10d_end_moving\", \"20d_end_moving\", \"60d_end_moving\", \"120d_end_moving\", \"180d_end_moving\", \"240d_end_moving\", \"all_end_moving\", \"lower_band\", \"higher_band\"]].iplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7d[[\"start_price\", \"end_price\", \"max_price\", \"avg_price\", \"min_price\"]].iplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1m[[\"start_price\", \"end_price\", \"max_price\", \"avg_price\", \"min_price\"]].iplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-layer LSTM Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1d = df_1d[[ 'count', 'sign', 'amount', 'total_price', 'market_cap', \n",
    "       'start_price', 'end_price', 'max_price', 'avg_price','min_price',\n",
    "       '5d_end_moving', '10d_end_moving', '20d_end_moving', '60d_end_moving', '120d_end_moving',\n",
    "       '180d_end_moving', '240d_end_moving', 'all_end_moving', 'lower_band', 'higher_band',\n",
    "       'rsi']]\n",
    "df_1d.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Month2Week_Reduction() :\n",
    "    def __init__(self, sess, name, input_dim=21, target_dim=9, num_layer=3, hidden_dim=21, month=30, week=7) : \n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self.input_dim = input_dim\n",
    "        self.target_dim = target_dim\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.month = month\n",
    "        self.week = week\n",
    "\n",
    "        \n",
    "    def LSTMCell(self) :\n",
    "        with tf.variable_scope(self.name):\n",
    "            ###################################################\n",
    "            ### input\n",
    "            # encoder_input_data : encoder input\n",
    "            # decoder_input_data : decoder input\n",
    "            # decoder_output_data : decdoer target\n",
    "            # keep_prob : dropout rate\n",
    "            # init : initial state of Encoder\n",
    "\n",
    "            self.encoder_input_data = tf.placeholder(tf.float32, [self.month, 1, self.input_dim], name=\"encoder_input\")\n",
    "            self.decoder_input_data = tf.placeholder(tf.float32, [self.week, 1, self.input_dim-1], name=\"decoder_input\")\n",
    "            self.decoder_output_data = tf.placeholder(tf.float32, [self.week, 1, self.target_dim], name=\"decoder_output\")\n",
    "            self.keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "            self.is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "            self.before_day = tf.placeholder(tf.float32, name = \"before_day\")\n",
    "            self.reduction_init_point = tf.placeholder(tf.float32, name = \"init_point\")\n",
    "            self.learning_rate = tf.placeholder(tf.float32, name = \"learning_rate\")\n",
    "\n",
    "            self.init11 = tf.constant(np.zeros([1, self.hidden_dim],np.float32), dtype=tf.float32)\n",
    "            self.init12 = tf.constant(np.zeros([1, self.hidden_dim],np.float32), dtype=tf.float32)\n",
    "            self.init13 = tf.constant(np.zeros([1, self.hidden_dim],np.float32), dtype=tf.float32)\n",
    "            self.init21 = tf.constant(np.zeros([1, self.hidden_dim],np.float32), dtype=tf.float32)\n",
    "            self.init22 = tf.constant(np.zeros([1, self.hidden_dim],np.float32), dtype=tf.float32)\n",
    "            self.init23 = tf.constant(np.zeros([1, self.hidden_dim],np.float32), dtype=tf.float32)\n",
    "            ###################################################\n",
    "\n",
    "\n",
    "            ###################################################\n",
    "            ### Encoder params\n",
    "            # EWx : matrix for input, [num_layer, 4, hidden_dim, input_dim]\n",
    "            # EWh : matrix for hidden state, [num_layer, 4, hidden_dim, hidden_dim]\n",
    "            # b : bias, [num_layer, 4, hidden_dim]\n",
    "            # 4 = forget(f)/input(i)/output(o) gate + cell input activation function(g)\n",
    "            # initial_state : [2, self.num_layer, self.hidden_dim]\n",
    "\n",
    "            EWx = tf.get_variable('EWx',shape=[self.num_layer, 4, self.input_dim, self.hidden_dim], initializer=tf.contrib.layers.xavier_initializer())  \n",
    "            EWh = tf.get_variable('EWh', shape=[self.num_layer, 4, self.hidden_dim, self.hidden_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            Eb = tf.get_variable('Eb', shape=[self.num_layer, 4, self.hidden_dim], initializer=tf.constant_initializer(0.))\n",
    "\n",
    "            initial_state = [[self.init11, self.init12, self.init13], [self.init21, self.init22, self.init23]]\n",
    "            ###################################################\n",
    "\n",
    "\n",
    "            ###################################################\n",
    "            ### Encoder\n",
    "            # previous : previous state. [ct, ht]\n",
    "            # x : input data\n",
    "            # return : current state. [ct, ht]\n",
    "            # f/i/o : forget/input/output gate\n",
    "            # g : input information\n",
    "\n",
    "            def encoder(previous, x) :\n",
    "                c, h = previous\n",
    "                c_lst = []\n",
    "                h_lst = []\n",
    "\n",
    "                for idx in range(self.num_layer) :\n",
    "                    f = tf.sigmoid(tf.matmul(x, EWx[idx][0]) + tf.matmul(h[idx],EWh[idx][0]) + Eb[idx][0])\n",
    "                    i = tf.sigmoid(tf.matmul(x, EWx[idx][1]) + tf.matmul(h[idx],EWh[idx][1]) + Eb[idx][1])\n",
    "                    o = tf.sigmoid(tf.matmul(x, EWx[idx][2]) + tf.matmul(h[idx],EWh[idx][2]) + Eb[idx][2])\n",
    "                    g = tf.tanh(tf.matmul(x, EWx[idx][3]) + tf.matmul(h[idx],EWh[idx][3]) + Eb[idx][3])\n",
    "                    c_t = c[idx]*f + g*i \n",
    "                    h_t = tf.tanh(c[idx])*o\n",
    "                    x = h_t\n",
    "\n",
    "                    c_t.set_shape([1, 21])\n",
    "                    h_t.set_shape([1, 21])\n",
    "                    c_lst.append(c_t)\n",
    "                    h_lst.append(h_t)\n",
    "\n",
    "                #c_output = tf.Variable(c_lst, trainable=False, dtype=tf.float32)\n",
    "                #h_output = tf.Variable(h_lst, trainable=False, dtype=tf.float32)\n",
    "                return [c_lst, h_lst]\n",
    "\n",
    "            encoder_state = tf.scan(encoder, self.encoder_input_data, initializer=initial_state)\n",
    "            c_lst, h_lst = encoder_state\n",
    "            c = c_lst[-1]\n",
    "            h = h_lst[-1]\n",
    "            encoder_state = [c, h]\n",
    "            ###################################################\n",
    "\n",
    "\n",
    "            ##############################################3####\n",
    "            ### Encoder state embedding\n",
    "            Embedding_Wc = tf.get_variable('Embedding_Wc', shape=[self.hidden_dim, self.hidden_dim-1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            Embedding_Wh = tf.get_variable('Embedding_Wh',shape=[self.input_dim, self.hidden_dim-1], initializer=tf.contrib.layers.xavier_initializer())  \n",
    "\n",
    "            embedding_c_lst = []\n",
    "            embedding_h_lst = []\n",
    "\n",
    "            cc, hh = encoder_state\n",
    "            for idx in range(self.num_layer) :\n",
    "                new_c = tf.matmul(cc[idx], Embedding_Wc)\n",
    "                new_h = tf.matmul(hh[idx], Embedding_Wh)\n",
    "\n",
    "                new_c.set_shape([1, self.hidden_dim-1])\n",
    "                new_h.set_shape([1, self.hidden_dim-1])\n",
    "                embedding_c_lst.append(new_c)\n",
    "                embedding_h_lst.append(new_h)\n",
    "\n",
    "            encoder_state = [embedding_c_lst, embedding_h_lst]\n",
    "            ###################################################\n",
    "\n",
    "\n",
    "            ###################################################\n",
    "            ### Decoder params\n",
    "            # DWx : matrix for input, [num_layer, 4, hidden_dim, input_dim]\n",
    "            # DWh : matrix for hidden state, [num_layer, 4, hidden_dim, hidden_dim]\n",
    "            # Db : bias, [num_layer, 4, hidden_dim]\n",
    "            # DWp : FC network weight\n",
    "            # Dbp : FC network bias\n",
    "            # 4 = forget(f)/input(i)/output(o) gate + cell input activation function(g)\n",
    "            # initial_state : [2, self.num_layer, self.hidden_dim]\n",
    "\n",
    "            DWx = tf.get_variable('DWx', shape=[self.num_layer, 4, self.input_dim-1, self.hidden_dim-1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            DWh = tf.get_variable('DWh', shape=[self.num_layer, 4, self.input_dim-1, self.hidden_dim-1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            Db = tf.get_variable('Db', shape=[self.num_layer, 4, self.hidden_dim-1], initializer=tf.constant_initializer(0.))\n",
    "\n",
    "            DWp1 = tf.get_variable('DWp1', shape=[self.hidden_dim-1, 16], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            Dbp1 = tf.Variable(tf.random_normal([16]), name=\"Dbp1\")\n",
    "            DWp2 = tf.get_variable('DWp2', shape=[16, self.target_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            Dbp2 = tf.Variable(tf.random_normal([self.target_dim]), name=\"Dbp2\")\n",
    "            ###################################################\n",
    "\n",
    "\n",
    "            ###################################################\n",
    "            ### Decoder\n",
    "            # previous : previous state. [ct, ht, output]\n",
    "            # x : input data\n",
    "            # return : current state. [ct, ht, output]\n",
    "            # f/i/o : forget/input/output gate\n",
    "            # g : input information\n",
    "            def decoder(c, h, x) :\n",
    "                c_lst = []\n",
    "                h_lst = []\n",
    "\n",
    "                for idx in range(self.num_layer) :\n",
    "                    f = tf.sigmoid(tf.matmul(x, DWx[idx][0]) + tf.matmul(h[idx],DWh[idx][0]) + Db[idx][0])\n",
    "                    i = tf.sigmoid(tf.matmul(x, DWx[idx][1]) + tf.matmul(h[idx],DWh[idx][1]) + Db[idx][1])\n",
    "                    o = tf.sigmoid(tf.matmul(x, DWx[idx][2]) + tf.matmul(h[idx],DWh[idx][2]) + Db[idx][2])\n",
    "                    g = tf.tanh(tf.matmul(x, DWx[idx][3]) + tf.matmul(h[idx],DWh[idx][3]) + Db[idx][3])\n",
    "                    c_t = c[idx]*f + g*i\n",
    "                    h_t = tf.tanh(c[idx])*o\n",
    "                    x = h_t\n",
    "\n",
    "                    c_t.set_shape([1, self.hidden_dim-1])\n",
    "                    h_t.set_shape([1, self.hidden_dim-1])\n",
    "                    c_lst.append(c_t)\n",
    "                    h_lst.append(h_t)\n",
    "\n",
    "                Dlayer = tf.nn.relu(tf.matmul(h_t, DWp1) + Dbp1)\n",
    "                Dlayer = tf.nn.dropout(Dlayer, keep_prob=self.keep_prob)\n",
    "                output = tf.matmul(Dlayer, DWp2) + Dbp2\n",
    "\n",
    "                return [c_lst, h_lst, output]\n",
    "            ###################################################\n",
    "\n",
    "\n",
    "            ###################################################\n",
    "            ### Train & Test\n",
    "            output_lst = []\n",
    "            decoder_data = self.decoder_input_data[0]\n",
    "            c, h = encoder_state\n",
    "\n",
    "            def true_function1(idx) :\n",
    "                return self.decoder_input_data[idx]\n",
    "            def false_function1(output, previous, idx) :\n",
    "                return self.making_subindex_for_next_input(output, previous, idx)\n",
    "\n",
    "            for idx in range(self.week) :\n",
    "                c, h, output = decoder(c, h, decoder_data)\n",
    "                output_lst.append(output)\n",
    "\n",
    "                previous = decoder_data\n",
    "                decoder_data = tf.cond(self.is_training, lambda : true_function1(idx), lambda : false_function1(output, previous, idx))\n",
    "                \n",
    "            prediction = tf.stack(output_lst)\n",
    "            prediction.set_shape([self.week, 1, self.target_dim])\n",
    "            \n",
    "            self.prediction = prediction\n",
    "            self.cost = tf.reduce_sum(tf.square(self.prediction - self.decoder_output_data)) * self.cost_reduction(self.before_day)\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.cost)\n",
    "            ###################################################\n",
    "            \n",
    "    def cost_reduction(self, day) :\n",
    "        def reduction_function(day) :\n",
    "            rate = -(1.018)**(-day) + 1\n",
    "            return rate\n",
    "            \n",
    "        reduction = tf.cond(day>=self.reduction_init_point, lambda : reduction_function(day), lambda : reduction_function(self.reduction_init_point))\n",
    "        return reduction\n",
    "        \n",
    "    def making_subindex_for_next_input(self, result, previous, idx) :\n",
    "        # input : ['count', 'sign', 'amount', 'total_price', 'market_cap', 'end_price', 'max_price', 'avg_price', 'min_price']\n",
    "        # return : df_1d without end_price\n",
    "        result = result[0]\n",
    "        previous = previous[0]\n",
    "        \n",
    "        price = result[5]\n",
    "        d5 = (previous[9]*(5+idx) + price) / (5+idx+1)\n",
    "        d10 = (previous[10]*(10+idx) + price) / (10+idx+1)\n",
    "        d20 = (previous[11]*(20+idx) + price) / (20+idx+1)\n",
    "        d60 = (previous[12]*(60+idx) + price) / (60+idx+1)\n",
    "        d120 = (previous[13]*(120+idx) + price) / (120+idx+1)\n",
    "        d180 = (previous[14]*(180+idx) + price) / (180+idx+1)\n",
    "        d240 = (previous[15]*(240+idx) + price) / (240+idx+1)\n",
    "        dall = (previous[16]*(380+idx) + price) / (380+idx+1)\n",
    "        \n",
    "        high = previous[17]\n",
    "        low = previous[18]\n",
    "        m = (high+low) / 2\n",
    "        std = (high-low) / 4\n",
    "        new_m = (((high+low)/2)*(20+idx) + price) / (20+idx+1)\n",
    "        new_std = ((std*std + m*m)*(20+idx) + price*price) / (20+idx+1) - new_m*new_m\n",
    "        lower_band = new_m + 2*new_std\n",
    "        higher_band = new_m - 2*new_std\n",
    "        \n",
    "        def true_function2(rsi, rs) :\n",
    "            rs = tf.cond(rsi>50, lambda : rs+0.2, lambda : rs+0.1)             \n",
    "            rsi = 100 - 100*(1/(1+rs))\n",
    "            \n",
    "            return rsi\n",
    "        \n",
    "        def false_function2(rsi, rs) :\n",
    "            rs = tf.cond(rsi>50, lambda : rs-0.2, lambda : rs-0.1)    \n",
    "            rsi = 100 - 100*(1/(1+rs))\n",
    "            \n",
    "            return rsi\n",
    "        \n",
    "        rsi =  previous[19]\n",
    "        rs = 100/(100-rsi) -1\n",
    "        diff = previous[5] - price\n",
    "        rsi = tf.cond(diff > 0, lambda :  true_function2(rsi, rs), lambda : false_function2(rsi, rs))\n",
    "        \n",
    "        sub_index = tf.stack([d5, d10, d20, d60, d120, d180, d240, dall, higher_band, lower_band, rsi], axis=0)\n",
    "        \n",
    "        answer =  tf.concat([[result], [sub_index]], 1)\n",
    "        return answer\n",
    "            \n",
    "    def min_max_scaler(self, data):\n",
    "        numerator = data - tf.reduce_min(data, axis=0)\n",
    "        denominator = tf.reduce_max(data, axis=0) - tf.reduce_min(data, axis=0)\n",
    "        result = numerator / (denominator + tf.constant(1e-7, dtype=tf.float32))\n",
    "        return result\n",
    "    \n",
    "    def predict(self, encoder_input, decoder_input, keep_prop=1.0):\n",
    "        return self.sess.run(self.prediction, feed_dict={self.encoder_input_data: encoder_input, self.decoder_input_data: decoder_input, self.keep_prob: keep_prop, self.is_training : False})  \n",
    "\n",
    "    def train(self, encoder_input, decoder_input, decoder_output, before_day, init_point, learning_rate, keep_prop=0.7):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={self.encoder_input_data: encoder_input, self.decoder_input_data: decoder_input, self.decoder_output_data: decoder_output, self.keep_prob: keep_prop, self.is_training: True, self.before_day: before_day, self.reduction_init_point: init_point, self.learning_rate: learning_rate}) \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Month2Week_SmoothReduction() :\n",
    "    def __init__(self, sess, name, input_dim=21, target_dim=9, num_layer=3, hidden_dim=21, month=30, week=7) : \n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self.input_dim = input_dim\n",
    "        self.target_dim = target_dim\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.month = month\n",
    "        self.week = week\n",
    "        \n",
    "        \n",
    "    def LSTMCell(self) :\n",
    "        with tf.variable_scope(self.name):\n",
    "            ###################################################\n",
    "            ### input\n",
    "            # encoder_input_data : encoder input\n",
    "            # decoder_input_data : decoder input\n",
    "            # decoder_output_data : decdoer target\n",
    "            # keep_prob : dropout rate\n",
    "            # init : initial state of Encoder\n",
    "\n",
    "            self.encoder_input_data = tf.placeholder(tf.float32, [self.month, 1, self.input_dim], name=\"encoder_input\")\n",
    "            self.decoder_input_data = tf.placeholder(tf.float32, [self.week, 1, self.input_dim-1], name=\"decoder_input\")\n",
    "            self.decoder_output_data = tf.placeholder(tf.float32, [self.week, 1, self.target_dim], name=\"decoder_output\")\n",
    "            self.keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "            self.is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "            self.before_day = tf.placeholder(tf.float32, name = \"before_day\")\n",
    "            self.reduction_init_point = tf.placeholder(tf.float32, name = \"init_point\")\n",
    "            self.learning_rate = tf.placeholder(tf.float32, name = \"learning_rate\")\n",
    "\n",
    "            self.init11 = tf.constant(np.zeros([1, self.hidden_dim],np.float32), dtype=tf.float32)\n",
    "            self.init12 = tf.constant(np.zeros([1, self.hidden_dim],np.float32), dtype=tf.float32)\n",
    "            self.init13 = tf.constant(np.zeros([1, self.hidden_dim],np.float32), dtype=tf.float32)\n",
    "            self.init21 = tf.constant(np.zeros([1, self.hidden_dim],np.float32), dtype=tf.float32)\n",
    "            self.init22 = tf.constant(np.zeros([1, self.hidden_dim],np.float32), dtype=tf.float32)\n",
    "            self.init23 = tf.constant(np.zeros([1, self.hidden_dim],np.float32), dtype=tf.float32)\n",
    "            ###################################################\n",
    "\n",
    "\n",
    "            ###################################################\n",
    "            ### Encoder params\n",
    "            # EWx : matrix for input, [num_layer, 4, hidden_dim, input_dim]\n",
    "            # EWh : matrix for hidden state, [num_layer, 4, hidden_dim, hidden_dim]\n",
    "            # b : bias, [num_layer, 4, hidden_dim]\n",
    "            # 4 = forget(f)/input(i)/output(o) gate + cell input activation function(g)\n",
    "            # initial_state : [2, self.num_layer, self.hidden_dim]\n",
    "\n",
    "            EWx = tf.get_variable('EWx',shape=[self.num_layer, 4, self.input_dim, self.hidden_dim], initializer=tf.contrib.layers.xavier_initializer())  \n",
    "            EWh = tf.get_variable('EWh', shape=[self.num_layer, 4, self.hidden_dim, self.hidden_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            Eb = tf.get_variable('Eb', shape=[self.num_layer, 4, self.hidden_dim], initializer=tf.constant_initializer(0.))\n",
    "\n",
    "            initial_state = [[self.init11, self.init12, self.init13], [self.init21, self.init22, self.init23]]\n",
    "            ###################################################\n",
    "\n",
    "\n",
    "            ###################################################\n",
    "            ### Encoder\n",
    "            # previous : previous state. [ct, ht]\n",
    "            # x : input data\n",
    "            # return : current state. [ct, ht]\n",
    "            # f/i/o : forget/input/output gate\n",
    "            # g : input information\n",
    "\n",
    "            def encoder(previous, x) :\n",
    "                c, h = previous\n",
    "                c_lst = []\n",
    "                h_lst = []\n",
    "\n",
    "                for idx in range(self.num_layer) :\n",
    "                    f = tf.sigmoid(tf.matmul(x, EWx[idx][0]) + tf.matmul(h[idx],EWh[idx][0]) + Eb[idx][0])\n",
    "                    i = tf.sigmoid(tf.matmul(x, EWx[idx][1]) + tf.matmul(h[idx],EWh[idx][1]) + Eb[idx][1])\n",
    "                    o = tf.sigmoid(tf.matmul(x, EWx[idx][2]) + tf.matmul(h[idx],EWh[idx][2]) + Eb[idx][2])\n",
    "                    g = tf.tanh(tf.matmul(x, EWx[idx][3]) + tf.matmul(h[idx],EWh[idx][3]) + Eb[idx][3])\n",
    "                    c_t = c[idx]*f + g*i \n",
    "                    h_t = tf.tanh(c[idx])*o\n",
    "                    x = h_t\n",
    "\n",
    "                    c_t.set_shape([1, 21])\n",
    "                    h_t.set_shape([1, 21])\n",
    "                    c_lst.append(c_t)\n",
    "                    h_lst.append(h_t)\n",
    "\n",
    "                #c_output = tf.Variable(c_lst, trainable=False, dtype=tf.float32)\n",
    "                #h_output = tf.Variable(h_lst, trainable=False, dtype=tf.float32)\n",
    "                return [c_lst, h_lst]\n",
    "\n",
    "            encoder_state = tf.scan(encoder, self.encoder_input_data, initializer=initial_state)\n",
    "            c_lst, h_lst = encoder_state\n",
    "            c = c_lst[-1]\n",
    "            h = h_lst[-1]\n",
    "            encoder_state = [c, h]\n",
    "            ###################################################\n",
    "\n",
    "\n",
    "            ##############################################3####\n",
    "            ### Encoder state embedding\n",
    "            Embedding_Wc = tf.get_variable('Embedding_Wc', shape=[self.hidden_dim, self.hidden_dim-1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            Embedding_Wh = tf.get_variable('Embedding_Wh',shape=[self.input_dim, self.hidden_dim-1], initializer=tf.contrib.layers.xavier_initializer())  \n",
    "\n",
    "            embedding_c_lst = []\n",
    "            embedding_h_lst = []\n",
    "\n",
    "            cc, hh = encoder_state\n",
    "            for idx in range(self.num_layer) :\n",
    "                new_c = tf.matmul(cc[idx], Embedding_Wc)\n",
    "                new_h = tf.matmul(hh[idx], Embedding_Wh)\n",
    "\n",
    "                new_c.set_shape([1, self.hidden_dim-1])\n",
    "                new_h.set_shape([1, self.hidden_dim-1])\n",
    "                embedding_c_lst.append(new_c)\n",
    "                embedding_h_lst.append(new_h)\n",
    "\n",
    "            encoder_state = [embedding_c_lst, embedding_h_lst]\n",
    "            ###################################################\n",
    "\n",
    "\n",
    "            ###################################################\n",
    "            ### Decoder params\n",
    "            # DWx : matrix for input, [num_layer, 4, hidden_dim, input_dim]\n",
    "            # DWh : matrix for hidden state, [num_layer, 4, hidden_dim, hidden_dim]\n",
    "            # Db : bias, [num_layer, 4, hidden_dim]\n",
    "            # DWp : FC network weight\n",
    "            # Dbp : FC network bias\n",
    "            # 4 = forget(f)/input(i)/output(o) gate + cell input activation function(g)\n",
    "            # initial_state : [2, self.num_layer, self.hidden_dim]\n",
    "\n",
    "            DWx = tf.get_variable('DWx', shape=[self.num_layer, 4, self.input_dim-1, self.hidden_dim-1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            DWh = tf.get_variable('DWh', shape=[self.num_layer, 4, self.input_dim-1, self.hidden_dim-1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            Db = tf.get_variable('Db', shape=[self.num_layer, 4, self.hidden_dim-1], initializer=tf.constant_initializer(0.))\n",
    "\n",
    "            DWp1 = tf.get_variable('DWp1', shape=[self.hidden_dim-1, 16], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            Dbp1 = tf.Variable(tf.random_normal([16]), name=\"Dbp1\")\n",
    "            DWp2 = tf.get_variable('DWp2', shape=[16, self.target_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            Dbp2 = tf.Variable(tf.random_normal([self.target_dim]), name=\"Dbp2\")\n",
    "            ###################################################\n",
    "\n",
    "\n",
    "            ###################################################\n",
    "            ### Decoder\n",
    "            # previous : previous state. [ct, ht, output]\n",
    "            # x : input data\n",
    "            # return : current state. [ct, ht, output]\n",
    "            # f/i/o : forget/input/output gate\n",
    "            # g : input information\n",
    "            def decoder(c, h, x) :\n",
    "                c_lst = []\n",
    "                h_lst = []\n",
    "\n",
    "                for idx in range(self.num_layer) :\n",
    "                    f = tf.sigmoid(tf.matmul(x, DWx[idx][0]) + tf.matmul(h[idx],DWh[idx][0]) + Db[idx][0])\n",
    "                    i = tf.sigmoid(tf.matmul(x, DWx[idx][1]) + tf.matmul(h[idx],DWh[idx][1]) + Db[idx][1])\n",
    "                    o = tf.sigmoid(tf.matmul(x, DWx[idx][2]) + tf.matmul(h[idx],DWh[idx][2]) + Db[idx][2])\n",
    "                    g = tf.tanh(tf.matmul(x, DWx[idx][3]) + tf.matmul(h[idx],DWh[idx][3]) + Db[idx][3])\n",
    "                    c_t = c[idx]*f + g*i\n",
    "                    h_t = tf.tanh(c[idx])*o\n",
    "                    x = h_t\n",
    "\n",
    "                    c_t.set_shape([1, self.hidden_dim-1])\n",
    "                    h_t.set_shape([1, self.hidden_dim-1])\n",
    "                    c_lst.append(c_t)\n",
    "                    h_lst.append(h_t)\n",
    "\n",
    "                Dlayer = tf.nn.relu(tf.matmul(h_t, DWp1) + Dbp1)\n",
    "                Dlayer = tf.nn.dropout(Dlayer, keep_prob=self.keep_prob)\n",
    "                output = tf.matmul(Dlayer, DWp2) + Dbp2\n",
    "\n",
    "                return [c_lst, h_lst, output]\n",
    "            ###################################################\n",
    "\n",
    "\n",
    "            ###################################################\n",
    "            ### Train & Test\n",
    "            output_lst = []\n",
    "            decoder_data = self.decoder_input_data[0]\n",
    "            c, h = encoder_state\n",
    "\n",
    "            def true_function1(idx) :\n",
    "                return self.decoder_input_data[idx]\n",
    "            \n",
    "            def false_function1(output, previous, idx) :\n",
    "                return self.making_subindex_for_next_input(output, previous, idx)\n",
    "\n",
    "            for idx in range(self.week) :\n",
    "                c, h, output = decoder(c, h, decoder_data)\n",
    "                output_lst.append(output)\n",
    "\n",
    "                previous = decoder_data\n",
    "                decoder_data = tf.cond(self.is_training, lambda : true_function1(idx), lambda : false_function1(output, previous, idx))\n",
    "                \n",
    "            prediction = tf.stack(output_lst)\n",
    "            prediction.set_shape([self.week, 1, self.target_dim])\n",
    "            \n",
    "            self.prediction = prediction\n",
    "            self.cost = tf.reduce_sum(tf.square(self.prediction - self.decoder_output_data)) * self.cost_reduction(self.before_day)\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.cost)\n",
    "            ###################################################\n",
    "            \n",
    "    def cost_reduction(self, day) :\n",
    "        def reduction_function1(day) :\n",
    "            rate = -(1.018)**(-day) + 1\n",
    "            return rate\n",
    "        \n",
    "        def reduction_function2(day) :\n",
    "            rate = np.log(1.018) * (1.018)**(-self.reduction_init_point) * (day - self.reduction_init_point) + (reduction_function1(self.reduction_init_point))\n",
    "            return rate\n",
    "            \n",
    "        reduction = tf.cond(day>=80, lambda : reduction_function1(day), lambda : reduction_function2(day))\n",
    "        return reduction\n",
    "        \n",
    "    def making_subindex_for_next_input(self, result, previous, idx) :\n",
    "        # input : ['count', 'sign', 'amount', 'total_price', 'market_cap', 'end_price', 'max_price', 'avg_price', 'min_price']\n",
    "        # return : df_1d without end_price\n",
    "        result = result[0]\n",
    "        previous = previous[0]\n",
    "        \n",
    "        price = result[5]\n",
    "        d5 = (previous[9]*(5+idx) + price) / (5+idx+1)\n",
    "        d10 = (previous[10]*(10+idx) + price) / (10+idx+1)\n",
    "        d20 = (previous[11]*(20+idx) + price) / (20+idx+1)\n",
    "        d60 = (previous[12]*(60+idx) + price) / (60+idx+1)\n",
    "        d120 = (previous[13]*(120+idx) + price) / (120+idx+1)\n",
    "        d180 = (previous[14]*(180+idx) + price) / (180+idx+1)\n",
    "        d240 = (previous[15]*(240+idx) + price) / (240+idx+1)\n",
    "        dall = (previous[16]*(380+idx) + price) / (380+idx+1)\n",
    "        \n",
    "        high = previous[17]\n",
    "        low = previous[18]\n",
    "        m = (high+low) / 2\n",
    "        std = (high-low) / 4\n",
    "        new_m = (((high+low)/2)*(20+idx) + price) / (20+idx+1)\n",
    "        new_std = ((std*std + m*m)*(20+idx) + price*price) / (20+idx+1) - new_m*new_m\n",
    "        lower_band = new_m + 2*new_std\n",
    "        higher_band = new_m - 2*new_std\n",
    "        \n",
    "        def true_function2(rsi, rs) :\n",
    "            rs = tf.cond(rsi>50, lambda : rs+0.2, lambda : rs+0.1)             \n",
    "            rsi = 100 - 100*(1/(1+rs))\n",
    "            \n",
    "            return rsi\n",
    "        \n",
    "        def false_function2(rsi, rs) :\n",
    "            rs = tf.cond(rsi>50, lambda : rs-0.2, lambda : rs-0.1)    \n",
    "            rsi = 100 - 100*(1/(1+rs))\n",
    "            \n",
    "            return rsi\n",
    "        \n",
    "        rsi =  previous[19]\n",
    "        rs = 100/(100-rsi) -1\n",
    "        diff = previous[5] - price\n",
    "        rsi = tf.cond(diff > 0, lambda :  true_function2(rsi, rs), lambda : false_function2(rsi, rs))\n",
    "        \n",
    "        sub_index = tf.stack([d5, d10, d20, d60, d120, d180, d240, dall, higher_band, lower_band, rsi], axis=0)\n",
    "        \n",
    "        answer =  tf.concat([[result], [sub_index]], 1)\n",
    "        return answer\n",
    "            \n",
    "    def min_max_scaler(self, data):\n",
    "        numerator = data - tf.reduce_min(data, axis=0)\n",
    "        denominator = tf.reduce_max(data, axis=0) - tf.reduce_min(data, axis=0)\n",
    "        result = numerator / (denominator + tf.constant(1e-7, dtype=tf.float32))\n",
    "        return result\n",
    "    \n",
    "    def predict(self, encoder_input, decoder_input, keep_prop=1.0):\n",
    "        return self.sess.run(self.prediction, feed_dict={self.encoder_input_data: encoder_input, self.decoder_input_data: decoder_input, self.keep_prob: keep_prop, self.is_training : False})  \n",
    "\n",
    "    def train(self, encoder_input, decoder_input, decoder_output, before_day, init_point, learning_rate, keep_prop=0.7):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={self.encoder_input_data: encoder_input, self.decoder_input_data: decoder_input, self.decoder_output_data: decoder_output, self.keep_prob: keep_prop, self.is_training: True, self.before_day: before_day, self.reduction_init_point: init_point, self.learning_rate: learning_rate}) \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Month2Week() :\n",
    "    def __init__(self, sess, name, input_dim=21, target_dim=9, num_layer=3, hidden_dim=21,month=30, week=7) : \n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self.input_dim = input_dim\n",
    "        self.target_dim = target_dim\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.month = month\n",
    "        self.week = week\n",
    "        \n",
    "        \n",
    "    def LSTMCell(self) :\n",
    "        with tf.variable_scope(self.name):\n",
    "            ###################################################\n",
    "            ### input\n",
    "            # encoder_input_data : encoder input\n",
    "            # decoder_input_data : decoder input\n",
    "            # decoder_output_data : decdoer target\n",
    "            # keep_prob : dropout rate\n",
    "            # init : initial state of Encoder\n",
    "\n",
    "            self.encoder_input_data = tf.placeholder(tf.float32, [self.month, 1, self.input_dim], name=\"encoder_input\")\n",
    "            self.decoder_input_data = tf.placeholder(tf.float32, [self.week, 1, self.input_dim-1], name=\"decoder_input\")\n",
    "            self.decoder_output_data = tf.placeholder(tf.float32, [self.week, 1, self.target_dim], name=\"decoder_output\")\n",
    "            self.keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "            self.is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "            self.learning_rate = tf.placeholder(tf.float32, name = \"learning_rate\")\n",
    "\n",
    "            self.init11 = tf.constant(np.zeros([1, self.hidden_dim],np.float32), dtype=tf.float32)\n",
    "            self.init12 = tf.constant(np.zeros([1, self.hidden_dim],np.float32), dtype=tf.float32)\n",
    "            self.init13 = tf.constant(np.zeros([1, self.hidden_dim],np.float32), dtype=tf.float32)\n",
    "            self.init21 = tf.constant(np.zeros([1, self.hidden_dim],np.float32), dtype=tf.float32)\n",
    "            self.init22 = tf.constant(np.zeros([1, self.hidden_dim],np.float32), dtype=tf.float32)\n",
    "            self.init23 = tf.constant(np.zeros([1, self.hidden_dim],np.float32), dtype=tf.float32)\n",
    "            ###################################################\n",
    "\n",
    "\n",
    "            ###################################################\n",
    "            ### Encoder params\n",
    "            # EWx : matrix for input, [num_layer, 4, hidden_dim, input_dim]\n",
    "            # EWh : matrix for hidden state, [num_layer, 4, hidden_dim, hidden_dim]\n",
    "            # b : bias, [num_layer, 4, hidden_dim]\n",
    "            # 4 = forget(f)/input(i)/output(o) gate + cell input activation function(g)\n",
    "            # initial_state : [2, self.num_layer, self.hidden_dim]\n",
    "\n",
    "            EWx = tf.get_variable('EWx',shape=[self.num_layer, 4, self.input_dim, self.hidden_dim], initializer=tf.contrib.layers.xavier_initializer())  \n",
    "            EWh = tf.get_variable('EWh', shape=[self.num_layer, 4, self.hidden_dim, self.hidden_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            Eb = tf.get_variable('Eb', shape=[self.num_layer, 4, self.hidden_dim], initializer=tf.constant_initializer(0.))\n",
    "\n",
    "            initial_state = [[self.init11, self.init12, self.init13], [self.init21, self.init22, self.init23]]\n",
    "            ###################################################\n",
    "\n",
    "\n",
    "            ###################################################\n",
    "            ### Encoder\n",
    "            # previous : previous state. [ct, ht]\n",
    "            # x : input data\n",
    "            # return : current state. [ct, ht]\n",
    "            # f/i/o : forget/input/output gate\n",
    "            # g : input information\n",
    "\n",
    "            def encoder(previous, x) :\n",
    "                c, h = previous\n",
    "                c_lst = []\n",
    "                h_lst = []\n",
    "\n",
    "                for idx in range(self.num_layer) :\n",
    "                    f = tf.sigmoid(tf.matmul(x, EWx[idx][0]) + tf.matmul(h[idx],EWh[idx][0]) + Eb[idx][0])\n",
    "                    i = tf.sigmoid(tf.matmul(x, EWx[idx][1]) + tf.matmul(h[idx],EWh[idx][1]) + Eb[idx][1])\n",
    "                    o = tf.sigmoid(tf.matmul(x, EWx[idx][2]) + tf.matmul(h[idx],EWh[idx][2]) + Eb[idx][2])\n",
    "                    g = tf.tanh(tf.matmul(x, EWx[idx][3]) + tf.matmul(h[idx],EWh[idx][3]) + Eb[idx][3])\n",
    "                    c_t = c[idx]*f + g*i \n",
    "                    h_t = tf.tanh(c[idx])*o\n",
    "                    x = h_t\n",
    "\n",
    "                    c_t.set_shape([1, 21])\n",
    "                    h_t.set_shape([1, 21])\n",
    "                    c_lst.append(c_t)\n",
    "                    h_lst.append(h_t)\n",
    "\n",
    "                #c_output = tf.Variable(c_lst, trainable=False, dtype=tf.float32)\n",
    "                #h_output = tf.Variable(h_lst, trainable=False, dtype=tf.float32)\n",
    "                return [c_lst, h_lst]\n",
    "\n",
    "            encoder_state = tf.scan(encoder, self.encoder_input_data, initializer=initial_state)\n",
    "            c_lst, h_lst = encoder_state\n",
    "            c = c_lst[-1]\n",
    "            h = h_lst[-1]\n",
    "            encoder_state = [c, h]\n",
    "            ###################################################\n",
    "\n",
    "\n",
    "            ##############################################3####\n",
    "            ### Encoder state embedding\n",
    "            Embedding_Wc = tf.get_variable('Embedding_Wc', shape=[self.hidden_dim, self.hidden_dim-1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            Embedding_Wh = tf.get_variable('Embedding_Wh',shape=[self.input_dim, self.hidden_dim-1], initializer=tf.contrib.layers.xavier_initializer())  \n",
    "\n",
    "            embedding_c_lst = []\n",
    "            embedding_h_lst = []\n",
    "\n",
    "            cc, hh = encoder_state\n",
    "            for idx in range(self.num_layer) :\n",
    "                new_c = tf.matmul(cc[idx], Embedding_Wc)\n",
    "                new_h = tf.matmul(hh[idx], Embedding_Wh)\n",
    "\n",
    "                new_c.set_shape([1, self.hidden_dim-1])\n",
    "                new_h.set_shape([1, self.hidden_dim-1])\n",
    "                embedding_c_lst.append(new_c)\n",
    "                embedding_h_lst.append(new_h)\n",
    "\n",
    "            encoder_state = [embedding_c_lst, embedding_h_lst]\n",
    "            ###################################################\n",
    "\n",
    "\n",
    "            ###################################################\n",
    "            ### Decoder params\n",
    "            # DWx : matrix for input, [num_layer, 4, hidden_dim, input_dim]\n",
    "            # DWh : matrix for hidden state, [num_layer, 4, hidden_dim, hidden_dim]\n",
    "            # Db : bias, [num_layer, 4, hidden_dim]\n",
    "            # DWp : FC network weight\n",
    "            # Dbp : FC network bias\n",
    "            # 4 = forget(f)/input(i)/output(o) gate + cell input activation function(g)\n",
    "            # initial_state : [2, self.num_layer, self.hidden_dim]\n",
    "\n",
    "            DWx = tf.get_variable('DWx', shape=[self.num_layer, 4, self.input_dim-1, self.hidden_dim-1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            DWh = tf.get_variable('DWh', shape=[self.num_layer, 4, self.input_dim-1, self.hidden_dim-1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            Db = tf.get_variable('Db', shape=[self.num_layer, 4, self.hidden_dim-1], initializer=tf.constant_initializer(0.))\n",
    "\n",
    "            DWp1 = tf.get_variable('DWp1', shape=[self.hidden_dim-1, 16], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            Dbp1 = tf.Variable(tf.random_normal([16]), name=\"Dbp1\")\n",
    "            DWp2 = tf.get_variable('DWp2', shape=[16, self.target_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            Dbp2 = tf.Variable(tf.random_normal([self.target_dim]), name=\"Dbp2\")\n",
    "            ###################################################\n",
    "\n",
    "\n",
    "            ###################################################\n",
    "            ### Decoder\n",
    "            # previous : previous state. [ct, ht, output]\n",
    "            # x : input data\n",
    "            # return : current state. [ct, ht, output]\n",
    "            # f/i/o : forget/input/output gate\n",
    "            # g : input information\n",
    "            def decoder(c, h, x) :\n",
    "                c_lst = []\n",
    "                h_lst = []\n",
    "\n",
    "                for idx in range(self.num_layer) :\n",
    "                    f = tf.sigmoid(tf.matmul(x, DWx[idx][0]) + tf.matmul(h[idx],DWh[idx][0]) + Db[idx][0])\n",
    "                    i = tf.sigmoid(tf.matmul(x, DWx[idx][1]) + tf.matmul(h[idx],DWh[idx][1]) + Db[idx][1])\n",
    "                    o = tf.sigmoid(tf.matmul(x, DWx[idx][2]) + tf.matmul(h[idx],DWh[idx][2]) + Db[idx][2])\n",
    "                    g = tf.tanh(tf.matmul(x, DWx[idx][3]) + tf.matmul(h[idx],DWh[idx][3]) + Db[idx][3])\n",
    "                    c_t = c[idx]*f + g*i\n",
    "                    h_t = tf.tanh(c[idx])*o\n",
    "                    x = h_t\n",
    "\n",
    "                    c_t.set_shape([1, self.hidden_dim-1])\n",
    "                    h_t.set_shape([1, self.hidden_dim-1])\n",
    "                    c_lst.append(c_t)\n",
    "                    h_lst.append(h_t)\n",
    "\n",
    "                Dlayer = tf.nn.relu(tf.matmul(h_t, DWp1) + Dbp1)\n",
    "                Dlayer = tf.nn.dropout(Dlayer, keep_prob=self.keep_prob)\n",
    "                output = tf.matmul(Dlayer, DWp2) + Dbp2\n",
    "\n",
    "                return [c_lst, h_lst, output]\n",
    "            ###################################################\n",
    "\n",
    "\n",
    "            ###################################################\n",
    "            ### Train & Test\n",
    "            output_lst = []\n",
    "            decoder_data = self.decoder_input_data[0]\n",
    "            c, h = encoder_state\n",
    "\n",
    "            def true_function1(idx) :\n",
    "                return self.decoder_input_data[idx]\n",
    "            def false_function1(output, previous, idx) :\n",
    "                return self.making_subindex_for_next_input(output, previous, idx)\n",
    "\n",
    "            for idx in range(self.week) :\n",
    "                c, h, output = decoder(c, h, decoder_data)\n",
    "                output_lst.append(output)\n",
    "\n",
    "                previous = decoder_data\n",
    "                decoder_data = tf.cond(self.is_training, lambda : true_function1(idx), lambda : false_function1(output, previous, idx))\n",
    "\n",
    "            prediction = tf.stack(output_lst)\n",
    "            prediction.set_shape([self.week, 1, self.target_dim])\n",
    "            \n",
    "            self.prediction = prediction\n",
    "            self.cost = tf.reduce_sum(tf.square(self.prediction - self.decoder_output_data))\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.cost)\n",
    "            ###################################################\n",
    "            \n",
    "    def making_subindex_for_next_input(self, result, previous, idx) :\n",
    "        # input : ['count', 'sign', 'amount', 'total_price', 'market_cap', 'end_price', 'max_price', 'avg_price', 'min_price']\n",
    "        # return : df_1d without end_price\n",
    "        result = result[0]\n",
    "        previous = previous[0]\n",
    "        \n",
    "        price = result[5]\n",
    "        d5 = (previous[9]*(5+idx) + price) / (5+idx+1)\n",
    "        d10 = (previous[10]*(10+idx) + price) / (10+idx+1)\n",
    "        d20 = (previous[11]*(20+idx) + price) / (20+idx+1)\n",
    "        d60 = (previous[12]*(60+idx) + price) / (60+idx+1)\n",
    "        d120 = (previous[13]*(120+idx) + price) / (120+idx+1)\n",
    "        d180 = (previous[14]*(180+idx) + price) / (180+idx+1)\n",
    "        d240 = (previous[15]*(240+idx) + price) / (240+idx+1)\n",
    "        dall = (previous[16]*(380+idx) + price) / (380+idx+1)\n",
    "        \n",
    "        high = previous[17]\n",
    "        low = previous[18]\n",
    "        m = (high+low) / 2\n",
    "        std = (high-low) / 4\n",
    "        new_m = (((high+low)/2)*(20+idx) + price) / (20+idx+1)\n",
    "        new_std = ((std*std + m*m)*(20+idx) + price*price) / (20+idx+1) - new_m*new_m\n",
    "        lower_band = new_m + 2*new_std\n",
    "        higher_band = new_m - 2*new_std\n",
    "        \n",
    "        def true_function2(rsi, rs) :\n",
    "            rs = tf.cond(rsi>50, lambda : rs+0.2, lambda : rs+0.1)             \n",
    "            rsi = 100 - 100*(1/(1+rs))\n",
    "            \n",
    "            return rsi\n",
    "        \n",
    "        def false_function2(rsi, rs) :\n",
    "            rs = tf.cond(rsi>50, lambda : rs-0.2, lambda : rs-0.1)    \n",
    "            rsi = 100 - 100*(1/(1+rs))\n",
    "            \n",
    "            return rsi\n",
    "        \n",
    "        rsi =  previous[19]\n",
    "        rs = 100/(100-rsi) -1\n",
    "        diff = previous[5] - price\n",
    "        rsi = tf.cond(diff > 0, lambda :  true_function2(rsi, rs), lambda : false_function2(rsi, rs))\n",
    "        \n",
    "        sub_index = tf.stack([d5, d10, d20, d60, d120, d180, d240, dall, higher_band, lower_band, rsi], axis=0)\n",
    "        answer =  tf.concat([[result], [sub_index]], 1)\n",
    "        return answer\n",
    "            \n",
    "    def min_max_scaler(self, data):\n",
    "        \n",
    "        numerator = data - tf.reduce_min(data, axis=0)\n",
    "        denominator = tf.reduce_max(data, axis=0) - tf.reduce_min(data, axis=0)\n",
    "        result = numerator / (denominator + tf.constant(1e-7, dtype=tf.float32))\n",
    "        return result\n",
    "    \n",
    "    def predict(self, encoder_input, decoder_input, keep_prop=1.0):\n",
    "        return self.sess.run(self.prediction, feed_dict={self.encoder_input_data: encoder_input, self.decoder_input_data: decoder_input, self.keep_prob: keep_prop, self.is_training : False})  \n",
    "\n",
    "    def train(self, encoder_input, decoder_input, decoder_output, learning_rate, keep_prop=0.7): \n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={self.encoder_input_data: encoder_input, self.decoder_input_data: decoder_input, self.decoder_output_data: decoder_output, self.keep_prob: keep_prop, self.is_training: True, self.learning_rate : learning_rate}) \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "\n",
    "    return numerator / (denominator + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deNormalization(df, result) :\n",
    "    data = df.values\n",
    "    maximum = np.max(data, 0)\n",
    "    minimum = np.min(data, 0)\n",
    "    \n",
    "    return data * (maximum-minimum+1e-7) + minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_batch(df, idx) :\n",
    "    new_df = df.copy()\n",
    "    data = min_max_scaler(new_df.values)\n",
    "    encoder_input = np.asarray(data[idx:idx+30])\n",
    "    \n",
    "    data2 = []\n",
    "    for datum in data :\n",
    "        data2.append(np.concatenate([datum[:5], datum[6:]], 0))\n",
    "                     \n",
    "    decoder_input = np.asarray(data2[idx+29:idx+36])\n",
    "    \n",
    "    data3 = []\n",
    "    for datum in data2 :\n",
    "        data3.append(datum[:9])\n",
    "    \n",
    "    decoder_output = np.asarray(data3[idx+30:idx+37])\n",
    "    \n",
    "    scaled_encoder_input = encoder_input.reshape([30, 1, 21])\n",
    "    scaled_decoder_input = decoder_input.reshape([7, 1, 20])\n",
    "    scaled_decoder_output = decoder_output.reshape([7, 1, 9])\n",
    "    \n",
    "    return scaled_encoder_input, scaled_decoder_input, scaled_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_batch2(df,) :\n",
    "    new_df = df.copy()\n",
    "    length = len(new_df)\n",
    "    \n",
    "    data = min_max_scaler(new_df.values)\n",
    "    encoder_input = data[length-30:]\n",
    "    \n",
    "    data2 = []\n",
    "    for datum in data :\n",
    "        data2.append(np.concatenate([datum[:5], datum[6:]], 0))\n",
    "                     \n",
    "    decoder_input = data2[-1]\n",
    "    \n",
    "    scaled_encoder_input = encoder_input.reshape([30, 1, 21])\n",
    "    scaled_decoder_input = np.asarray([decoder_input] * 7).reshape([7, 1, 20])\n",
    "    \n",
    "    return scaled_encoder_input, scaled_decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_rate(epoch) :\n",
    "    if epoch < 2 :\n",
    "        return 0.003\n",
    "    \n",
    "    elif epoch < 3 :\n",
    "        return 0.002\n",
    "    \n",
    "    elif epoch < 40 :\n",
    "        return 0.001\n",
    "    \n",
    "    else :\n",
    "        return 0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyperparameter\n",
    "month = 30\n",
    "week = 7\n",
    "num_epoch = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "model1 = Month2Week(sess, \"model_no_reduction\")\n",
    "model1.LSTMCell()\n",
    "print('Model Generated!')\n",
    "\n",
    "model2 = Month2Week_Reduction(sess, \"model_reduction40\")\n",
    "model2.LSTMCell()\n",
    "model3 = Month2Week_Reduction(sess, \"model_reduction80\")\n",
    "model3.LSTMCell()\n",
    "model4 = Month2Week_Reduction(sess, \"model_reduction120\")\n",
    "model4.LSTMCell()\n",
    "print('Model_Reduction Generated!')\n",
    "\n",
    "model5 = Month2Week_SmoothReduction(sess, \"model_smooth_reduction40\")\n",
    "model5.LSTMCell()\n",
    "model6 = Month2Week_SmoothReduction(sess, \"model_smooth_reduction80\")\n",
    "model6.LSTMCell()\n",
    "model7 = Month2Week_SmoothReduction(sess, \"model_smooth_reduction120\")\n",
    "model7.LSTMCell()\n",
    "print('Model_SmoothReduction Generated!')\n",
    "\n",
    "cost_lst = []\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('Learning Ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Learning Started!')\n",
    "print(\" \")\n",
    "\n",
    "# train my model\n",
    "for epoch in range(num_epoch):\n",
    "    avg_cost1 = 0\n",
    "    avg_cost2 = 0\n",
    "    avg_cost3 = 0\n",
    "    avg_cost4 = 0\n",
    "    avg_cost5 = 0\n",
    "    avg_cost6 = 0\n",
    "    avg_cost7 = 0\n",
    "    total_batch = len(df)-(month+week)\n",
    "    print(\"epoch\", (epoch+1), \"Started\", end=\" \")\n",
    "    \n",
    "    for idx in range(total_batch):\n",
    "        batch_encoder, batch_decoder1, batch_decoder2 = making_batch(df, idx)\n",
    "        learning_rate = get_learning_rate(epoch)\n",
    "        \n",
    "        c1, _ = model1.train(batch_encoder, batch_decoder1, batch_decoder2, learning_rate)\n",
    "        c2, _ = model2.train(batch_encoder, batch_decoder1, batch_decoder2, idx, 40, learning_rate)\n",
    "        c3, _ = model3.train(batch_encoder, batch_decoder1, batch_decoder2, idx, 80, learning_rate)\n",
    "        c4, _ = model4.train(batch_encoder, batch_decoder1, batch_decoder2, idx, 120, learning_rate)\n",
    "        c5, _ = model5.train(batch_encoder, batch_decoder1, batch_decoder2, idx, 40, learning_rate)\n",
    "        c6, _ = model6.train(batch_encoder, batch_decoder1, batch_decoder2, idx, 80, learning_rate)\n",
    "        c7, _ = model7.train(batch_encoder, batch_decoder1, batch_decoder2, idx, 120, learning_rate)\n",
    "        \n",
    "        avg_cost1 += c1 / total_batch\n",
    "        avg_cost2 += c2 / total_batch\n",
    "        avg_cost3 += c3 / total_batch\n",
    "        avg_cost4 += c4 / total_batch\n",
    "        avg_cost5 += c5 / total_batch\n",
    "        avg_cost6 += c6 / total_batch\n",
    "        avg_cost7 += c7 / total_batch\n",
    "        \n",
    "        if idx%30 == 0 :\n",
    "            print(idx, end=\" \")\n",
    "            \n",
    "    cost_lst.append([avg_cost1,  avg_cost2, avg_cost3, avg_cost4, avg_cost5, avg_cost6, avg_cost7])\n",
    "            \n",
    "    print(\"\")\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost1 =', '{:.3f}'.format(avg_cost1))\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost2 =', '{:.3f}'.format(avg_cost2))\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost3 =', '{:.3f}'.format(avg_cost3))\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost4 =', '{:.3f}'.format(avg_cost4))\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost5 =', '{:.3f}'.format(avg_cost5))\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost6 =', '{:.3f}'.format(avg_cost6))\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost7 =', '{:.3f}'.format(avg_cost7))\n",
    "            \n",
    "    batch_encoder, batch_decoder = making_batch2(df)\n",
    "    r1 = model1.predict(batch_encoder, batch_decoder).reshape([7,9])\n",
    "    r2 = model2.predict(batch_encoder, batch_decoder).reshape([7,9])\n",
    "    r3 = model3.predict(batch_encoder, batch_decoder).reshape([7,9])\n",
    "    r4 = model4.predict(batch_encoder, batch_decoder).reshape([7,9])\n",
    "    r5 = model5.predict(batch_encoder, batch_decoder).reshape([7,9])\n",
    "    r6 = model6.predict(batch_encoder, batch_decoder).reshape([7,9])\n",
    "    r7 = model7.predict(batch_encoder, batch_decoder).reshape([7,9])\n",
    "\n",
    "    results = np.asarray([r1,r2,r3,r4,r5,r6,r7])\n",
    "    end_price_lst = []\n",
    "    for result in results :\n",
    "        temp = []\n",
    "    \n",
    "        for day in result :\n",
    "            temp.append(day[5])\n",
    "        end_price_lst.append(temp)\n",
    "        \n",
    "    result_df = pd.DataFrame.from_records(end_price_lst).T\n",
    "    result_df.to_csv(\"result{}.csv\".format(epoch))\n",
    "    print(\"prediction Saved!\")\n",
    "    print(\"\")\n",
    "    \n",
    "\n",
    "print(\"\")\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_encoder, batch_decoder = making_batch2(df)\n",
    "\n",
    "r1 = model1.predict(batch_encoder, batch_decoder).reshape([7,9])\n",
    "r2 = model2.predict(batch_encoder, batch_decoder).reshape([7,9])\n",
    "r3 = model3.predict(batch_encoder, batch_decoder).reshape([7,9])\n",
    "r4 = model4.predict(batch_encoder, batch_decoder).reshape([7,9])\n",
    "r5 = model5.predict(batch_encoder, batch_decoder).reshape([7,9])\n",
    "r6 = model6.predict(batch_encoder, batch_decoder).reshape([7,9])\n",
    "r7 = model7.predict(batch_encoder, batch_decoder).reshape([7,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = np.asarray([r1,r2,r3,r4,r5,r6,r7])\n",
    "end_price_lst = []\n",
    "\n",
    "for result in results :\n",
    "    temp = []\n",
    "    \n",
    "    for day in result :\n",
    "        temp.append(day[5])\n",
    "    end_price_lst.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame.from_records(end_price_lst).T\n",
    "result_df.iplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
