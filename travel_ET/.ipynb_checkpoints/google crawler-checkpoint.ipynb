{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup  \n",
    "from urllib.request import urlopen\n",
    "\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import geocoder\n",
    "import wikipedia as wk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install geocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# GoogleMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#셀리옴으로 크롤링하기\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "options.add_argument('window-size=1920x1080')\n",
    "#options.add_argument(\"disable-gpu\")\n",
    "options.add_argument(\"--disable-gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('C:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\selenium\\\\webdriver\\\\chrome\\\\chromedriver.exe', chrome_options=options)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#나라별 도시 크롤링\n",
    "country =  [\"스페인\", \"포르투갈\"]\n",
    "dic = {}\n",
    "for idx in country :\n",
    "    dic[idx] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "for nature in country :\n",
    "    base_url = 'https://www.google.co.kr/search?' +'q=' + nature + '+도시&hl=en'\n",
    "    driver.get(base_url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    lst = driver.find_elements_by_css_selector(\"a.klitem\")\n",
    "    for city in lst :\n",
    "        dic[nature].append(city.text.split(\"\\n\")[0])\n",
    "        tot += 1\n",
    "    \n",
    "print(tot) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regex = \">(.*)</span>\"\n",
    "result = {}\n",
    "no_coordi_lst = []\n",
    "err_lst = []\n",
    "check = 0\n",
    "\n",
    "for nature in country :\n",
    "    for place in dic[nature] :\n",
    "        tot = 0\n",
    "        check += 1\n",
    "        print(str(check) + \" \" + nature + \" \" + place, end = \" \")\n",
    "        \n",
    "        try : \n",
    "            base_url = 'https://www.google.co.kr/maps/search/' + place + '+관광지/data=!4m3!2m2!5m1!4e1'\n",
    "            driver.get(base_url)\n",
    "            time.sleep(5)\n",
    "\n",
    "            html =  driver.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "            latitude = 0\n",
    "            longtitude = 0 \n",
    "            url = driver.current_url\n",
    "\n",
    "            if \"/@\" in url :\n",
    "                url = re.search(\"/@(.*)z/\", url).group(1).split(\",\")\n",
    "                latitude = float(url[0])\n",
    "                longtitude = float(url[1])\n",
    "            else :\n",
    "                no_coordi_lst.append((place, nature))\n",
    "\n",
    "            place_lst = soup.select(\"body > jsl > div#app-container > div#content-container > div#pane > div.widget-pane.widget-pane-visible > div.widget-pane-content.scrollable-y > div.widget-pane-content-holder > div.section-listbox.section-listbox-root > div > div.section-result > div > div.section-result-header > div > h3\")\n",
    "            review_lst = soup.select(\"body > jsl > div#app-container > div#content-container > div#pane > div.widget-pane.widget-pane-visible > div.widget-pane-content.scrollable-y > div.widget-pane-content-holder > div.section-listbox.section-listbox-root > div > div.section-result > div > div.section-result-header > div > span > span.section-result-num-ratings\")\n",
    "            star_lst = soup.select(\"body > jsl > div#app-container > div#content-container > div#pane > div.widget-pane.widget-pane-visible > div.widget-pane-content.scrollable-y > div.widget-pane-content-holder > div.section-listbox.section-listbox-root > div > div.section-result > div > div.section-result-header > div > span > span > span\")\n",
    "\n",
    "            count = 0\n",
    "            for idx1, idx2, idx3 in zip(place_lst, review_lst, star_lst) :\n",
    "                result1 = re.search(regex, str(idx1)).group(1).split(\">\")[1]\n",
    "                result2 = re.search(regex, str(idx2)).group(1).replace(\"(\", \"\").replace(\")\" ,\"\").replace(\",\", \"\")\n",
    "                if not result2 :\n",
    "                    result2 = 0\n",
    "                else :\n",
    "                    result2 = int(result2)\n",
    "                result3 = float(re.search(regex, str(idx3)).group(1))\n",
    "\n",
    "                result[result1] = (place ,nature, result2, result3, latitude, longtitude)\n",
    "                count += 1\n",
    "                tot += 1\n",
    "\n",
    "            flag = True\n",
    "            if count != 20 :\n",
    "                flag = False\n",
    "\n",
    "            while flag :\n",
    "                try :\n",
    "                    next_button_id = 'section-pagination-button-next'\n",
    "                    inputElement = driver.find_element_by_id(next_button_id)\n",
    "                    inputElement.send_keys(\"\\n\")\n",
    "                    time.sleep(5)\n",
    "\n",
    "                    html = driver.page_source\n",
    "                    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "                    place_lst = soup.select(\"body > jsl > div#app-container > div#content-container > div#pane > div.widget-pane.widget-pane-visible > div.widget-pane-content.scrollable-y > div.widget-pane-content-holder > div.section-listbox.section-listbox-root > div > div.section-result > div > div.section-result-header > div > h3\")\n",
    "                    review_lst = soup.select(\"body > jsl > div#app-container > div#content-container > div#pane > div.widget-pane.widget-pane-visible > div.widget-pane-content.scrollable-y > div.widget-pane-content-holder > div.section-listbox.section-listbox-root > div > div.section-result > div > div.section-result-header > div > span > span.section-result-num-ratings\")\n",
    "                    star_lst = soup.select(\"body > jsl > div#app-container > div#content-container > div#pane > div.widget-pane.widget-pane-visible > div.widget-pane-content.scrollable-y > div.widget-pane-content-holder > div.section-listbox.section-listbox-root > div > div.section-result > div > div.section-result-header > div > span > span > span\")\n",
    "\n",
    "                    count = 0\n",
    "                    for idx1, idx2, idx3 in zip(place_lst, review_lst, star_lst) :\n",
    "                        result1 = re.search(regex, str(idx1)).group(1).split(\">\")[1]\n",
    "                        result2 = re.search(regex, str(idx2)).group(1).replace(\"(\", \"\").replace(\")\" ,\"\").replace(\",\", \"\")\n",
    "                        if not result2 :\n",
    "                            result2 = 0\n",
    "                        else :\n",
    "                            result2 = int(result2)\n",
    "                        result3 = float(re.search(regex, str(idx3)).group(1))\n",
    "\n",
    "                        result[result1] = (place ,nature, result2, result3, latitude, longtitude)\n",
    "                        count += 1\n",
    "                        tot += 1\n",
    "\n",
    "                    if count != 20 :\n",
    "                        flag = False\n",
    "\n",
    "                except :\n",
    "                    print(\"err1\", end=\" \")\n",
    "                    flag = False\n",
    "                    \n",
    "                    \n",
    "            print(tot, \"clear\")\n",
    "            \n",
    "        except :\n",
    "            err_lst.append((place, nature))\n",
    "            print(tot, \"err2\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_info = pd.DataFrame.from_items(list(result.items())).T\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# 구글 관광명소\n",
    "### (스페인 도시 검색 -> 스페인 ~~~ 여행 -> 관광명소 크롤링, 만약 관광명소 없으면 우측에서 크롤링 -> 없으면 예외)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#셀리옴으로 크롤링하기\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "options.add_argument('window-size=1920x1080')\n",
    "#options.add_argument(\"disable-gpu\")\n",
    "options.add_argument(\"--disable-gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('C:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\selenium\\\\webdriver\\\\chrome\\\\chromedriver.exe', chrome_options=options)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#나라별 도시 크롤링\n",
    "country =  [\"스페인\", \"포르투갈\"]\n",
    "dic = {}\n",
    "for idx in country :\n",
    "    dic[idx] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "for nature in country :\n",
    "    base_url = 'https://www.google.co.kr/search?' +'q=' + nature + '+도시'\n",
    "    driver.get(base_url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    lst = driver.find_elements_by_css_selector(\"a.klitem\")\n",
    "    for city in lst :\n",
    "        dic[nature].append(city.text.split(\"\\n\")[0])\n",
    "        tot += 1\n",
    "    \n",
    "print(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "err_lst = []\n",
    "\n",
    "regex1 = \">(.*)<\"\n",
    "regex2 = \"중 (.*)개\"\n",
    "regex3 = \"리뷰 (.*)개\"\n",
    "regex4 = \">(.*)<\"\n",
    "regex5 = \"\"\n",
    "\n",
    "tot = 1\n",
    "a = 0\n",
    "total = 0\n",
    "for nature in country :\n",
    "    for city in dic[nature] :\n",
    "        print(tot, nature, city, end = \" \")\n",
    "        a+=1\n",
    "        \n",
    "        try : \n",
    "            base_url = 'https://www.google.co.kr/search?' +'q=' + nature + '+' + city + '+' + \"관광지\"\n",
    "            driver.get(base_url)\n",
    "            time.sleep(5)\n",
    "            \n",
    "            more_button_id = '_d9n'\n",
    "            driver.find_element_by_id(more_button_id).click()\n",
    "            time.sleep(5)\n",
    "\n",
    "            html =  driver.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            temp = soup.select(\"body > div > div > div > ol > li > div > div\")\n",
    "            \n",
    "            total = 0\n",
    "            for place in temp :\n",
    "                soup = BeautifulSoup(str(place), \"html.parser\")\n",
    "                \n",
    "                name = re.search(regex1, str(soup.select(\"div > h2\")[0])).group(1)\n",
    "                \n",
    "                try : star = float(re.search(regex2, str(soup.select(\"div >  g-review-stars > span\")[0])).group(1))\n",
    "                except : star = 0\n",
    "                    \n",
    "                try : review = int(re.search(regex3, str(soup.select(\"div > div  > span._Mnc\")[0])).group(1).replace(\",\", \"\"))\n",
    "                except : review = 0\n",
    "                \n",
    "                try : short_info = re.search(regex4, str(soup.select(\"p._DAm\")[0])).group(1)\n",
    "                except : short_info = \"\"\n",
    "                    \n",
    "                try : long_info = str(soup.select(\"p._BAm\")[0]).split(\">\")[1].split(\"<\")[0]\n",
    "                except : long_info = \"\"\n",
    "                    \n",
    "                result.append((name, (nature, city, star, review, short_info, long_info)))\n",
    "                total += 1\n",
    "                \n",
    "            print(total, \"clear\")\n",
    "        except :\n",
    "            err_lst.append((nature, city))\n",
    "            print(\"err\")\n",
    "            \n",
    "        tot+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_google_info = pd.DataFrame.from_items(result).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_city_google_info))\n",
    "df_city_google_info.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#직접 url 들어가서 노가다\n",
    "#벨라스는 검색결과 없음\n",
    "err_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_city_google_info.to_csv('travel_google_info4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 지도받기\n",
    "### (geocoder 이용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('travel_google_info3.csv', encoding='cp949')\n",
    "result = result.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coordi_lst = []\n",
    "no_coordi_lst = []\n",
    "cnt = 0\n",
    "\n",
    "for desti in result :\n",
    "    target_place = desti[0]\n",
    "    target_city = desti[2]\n",
    "    target_nature = desti[1]\n",
    "    \n",
    "    if cnt%20 == 0 :\n",
    "        print(\"-----------------\",cnt,\"-----------------\")\n",
    "    cnt+=1\n",
    "    \n",
    "    latitude = 0\n",
    "    longtitude = 0 \n",
    "    \n",
    "    try :\n",
    "        g = geocoder.google(target_city + \" \" + target_place)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        trial = 0\n",
    "        while (g.latlng == None) :\n",
    "            g = geocoder.google(target_city + \" \" + target_place)\n",
    "            time.sleep(2)\n",
    "            trial += 1\n",
    "            \n",
    "            if trial >= 3 :\n",
    "                break\n",
    "                \n",
    "        latitude, longtitude = g.latlng\n",
    "        \n",
    "        coordi_lst.append((target_place, (target_nature, target_city, latitude, longtitude))) \n",
    "            \n",
    "    except :\n",
    "        no_coordi_lst.append((target_place, target_city, target_nature))\n",
    "        coordi_lst.append((target_place, (target_nature, target_city, latitude, longtitude))) \n",
    "        print(cnt, (target_place, target_city, target_nature))\n",
    "    \n",
    "    if cnt%400 == 0 :\n",
    "        df_place_coodi_info = pd.DataFrame.from_items(coordi_lst).T\n",
    "        df_place_coodi_info.to_csv('travel_google_coordinate' + str(cnt) + '.csv')\n",
    "        print(\"saved\")\n",
    "    \n",
    "df_place_coodi_info = pd.DataFrame.from_items(coordi_lst).T\n",
    "df_place_coodi_info.to_csv('travel_google_coordinate' + str(cnt) + '.csv')\n",
    "print(\"----------------- finish -----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_place_coodi_info = pd.DataFrame.from_items(coordi_lst).T\n",
    "df_place_coodi_info.to_csv('travel_google_coordinate.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 리뷰받기\n",
    "### (구글지도url에서 검색 ->리뷰보기->영어? 한국어? )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#셀리옴으로 크롤링하기\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "options.add_argument('window-size=1920x1080')\n",
    "#options.add_argument(\"disable-gpu\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "driver = webdriver.Chrome('C:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\selenium\\\\webdriver\\\\chrome\\\\chromedriver.exe', chrome_options=options)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('travel_google_info3.csv', encoding='cp949')\n",
    "df_lst = df.values.tolist()[60:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "err = []\n",
    "cnt = 60\n",
    "print(\"start\")\n",
    "\n",
    "for idx in df_lst :\n",
    "    place = idx[0]\n",
    "    nature = idx[1]\n",
    "    city = idx[2]\n",
    "    review_num = idx[4]\n",
    "    \n",
    "    if cnt%10 == 0 :\n",
    "        print(\"-----------------\",cnt,\"-----------------\")\n",
    "    cnt += 1\n",
    "    \n",
    "    #영어 리뷰를 받을거면 ?hl=en을 붙여줘야함\n",
    "    base_url = 'https://www.google.co.kr/maps/search/' + city + \" \" + place\n",
    "    driver.get(base_url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    #좌표받기\n",
    "    latitude = 0\n",
    "    longtitude = 0 \n",
    "    \n",
    "    url = driver.current_url\n",
    "    if \"/@\" in url :\n",
    "        try :\n",
    "            url = re.search(\"/@(.*)z/\", url).group(1).split(\",\")\n",
    "            latitude = float(url[0])\n",
    "            longtitude = float(url[1])\n",
    "            \n",
    "        except :\n",
    "            latitude = url\n",
    "            longtitude = url\n",
    "            err.append((nature, place, city, url))\n",
    "            print(cnt, \"url error\", (nature, place, city))\n",
    "            \n",
    "    else :\n",
    "        err.append((nature, place, city, url))\n",
    "        print(cnt, \"no coordinate\", (nature, place, city))\n",
    "\n",
    "    #리뷰받기\n",
    "    if review_num == 0 :\n",
    "        result.append([place, nature, city, (latitude, longtitude),[\"\"]])\n",
    "        continue\n",
    "        \n",
    "    time.sleep(3)\n",
    "    \n",
    "    flag = False\n",
    "    try :\n",
    "        driver.find_element_by_css_selector('button.widget-pane-link').click()\n",
    "        time.sleep(3)\n",
    "\n",
    "    except :\n",
    "        try :\n",
    "            driver.find_element_by_xpath(\"//div[@data-result-index='1'][@class='section-result']\").click()\n",
    "            print(cnt, \"no found\", (nature, place, city))\n",
    "            time.sleep(3)\n",
    "            flag = True\n",
    "            \n",
    "        except :\n",
    "            print(cnt, \"no review button\", (nature, place, city))\n",
    "            result.append([place, nature, city, (latitude, longtitude),[\"\"]])\n",
    "            err.append((nature, place, city, url))\n",
    "            continue\n",
    "        \n",
    "    if flag : \n",
    "        try :\n",
    "            driver.find_element_by_css_selector('button.widget-pane-link').click()\n",
    "            time.sleep(3)\n",
    "            \n",
    "        except :\n",
    "            print(cnt, \"no review button\", (nature, place, city))\n",
    "            result.append([place, nature, city, (latitude, longtitude),[\"\"]])\n",
    "            err.append((nature, place, city, url))\n",
    "            continue\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    for idx in range(10) :\n",
    "        try :\n",
    "            driver.find_element_by_css_selector('div.section-loading.noprint').click()\n",
    "            time.sleep(3)\n",
    "        except :\n",
    "            break\n",
    "    \n",
    "    html =  driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    review_lst = soup.select('body > jsl > div > div > div > div > div > div > div > div > div > div > div > div > div > div > span')\n",
    "    \n",
    "    if len(review_lst) == 0 :\n",
    "        result.append([place, nature, city, (latitude, longtitude),[\"\"]])\n",
    "    else :\n",
    "        temp = []\n",
    "        for review in review_lst :\n",
    "            temp.append(str(review).split('>')[1].split('<')[0])\n",
    "        \n",
    "        result.append([place, nature, city, (latitude, longtitude), temp])\n",
    "        \n",
    "    if cnt%100 == 0 :\n",
    "        \n",
    "        new_result = list(map(lambda x : (x[0], (x[1], x[2], x[3], str(x[4]))), result))\n",
    "        \n",
    "        df_place_review = pd.DataFrame.from_items(new_result).T\n",
    "        df_place_review.to_csv('travel_google_review' + str(cnt) + '.csv')\n",
    "        print(\"saved\")\n",
    "\n",
    "        \n",
    "\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_result = list(map(lambda x : (x[0], (x[1], x[2], x[3], str(x[4]))), result))\n",
    "df_place_review = pd.DataFrame.from_items(new_result).T\n",
    "err_lst = list(map(lambda x : (x[1],(x[0], x[2], x[3][:2])), err))\n",
    "df_err = pd.DataFrame.from_items(err_lst).T\n",
    "\n",
    "df_place_review.to_csv('travel_google_review_fin.csv')\n",
    "df_err.to_csv('google_review_err.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 위키피디아(영문)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영문 관광지 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#셀리옴으로 크롤링하기\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "options.add_argument('window-size=1920x1080')\n",
    "#options.add_argument(\"disable-gpu\")\n",
    "options.add_argument(\"--disable-gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('C:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\selenium\\\\webdriver\\\\chrome\\\\chromedriver.exe', chrome_options=options)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#나라별 도시 크롤링\n",
    "country =  [\"스페인\", \"포르투갈\"]\n",
    "dic = {}\n",
    "for idx in country :\n",
    "    dic[idx] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "for nature in country :\n",
    "    base_url = 'https://www.google.co.kr/search?' +'q=' + nature + '+도시&hl=en'\n",
    "    driver.get(base_url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    lst = driver.find_elements_by_css_selector(\"a.klitem\")\n",
    "    for city in lst :\n",
    "        dic[nature].append(city.text.split(\"\\n\")[0])\n",
    "        tot += 1\n",
    "    \n",
    "print(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "err_lst = []\n",
    "\n",
    "regex1 = \">(.*)<\"\n",
    "regex2 = \"Rated (.*) out\"\n",
    "regex3 = \"\\((.*)\\)\"\n",
    "regex4 = \">(.*)<\"\n",
    "regex5 = \"\"\n",
    "\n",
    "tot = 1\n",
    "a = 0\n",
    "total = 0\n",
    "for nature in country :\n",
    "    for city in dic[nature] :\n",
    "        print(tot, nature, city)\n",
    "        a+=1\n",
    "        \n",
    "        try : \n",
    "            #hl=en 을 넣어주면 영어 검색\n",
    "            base_url = 'https://www.google.co.kr/search?' +'q=' + nature + '+' + city + '+' + \"관광지&hl=en\"\n",
    "            driver.get(base_url)\n",
    "            time.sleep(5)\n",
    "            \n",
    "            #mor_buttin_id가 수시로 바뀌니 체크하기\n",
    "            more_button_id = '_x7n'\n",
    "            driver.find_element_by_class_name(more_button_id).click()\n",
    "            time.sleep(5)\n",
    "\n",
    "            html =  driver.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            temp = soup.select(\"body > div > div > div > ol > li > div > div\")\n",
    "            \n",
    "            total = 0\n",
    "            for idx, place in enumerate(temp) :\n",
    "                soup = BeautifulSoup(str(place), \"html.parser\")\n",
    "                \n",
    "                name = re.search(regex1, str(soup.select(\"div > h2\")[0])).group(1)\n",
    "                \n",
    "                try : star = float(re.search(regex2, str(soup.select(\"div >  g-review-stars > span\")[0])).group(1))\n",
    "                except : star = 0\n",
    "                    \n",
    "                try : review = int(re.search(regex3, str(soup.select(\"div > div  > span._Mnc\")[0])).group(1).replace(\",\", \"\"))\n",
    "                except : review = 0\n",
    "                \n",
    "                try : short_info = re.search(regex4, str(soup.select(\"p._DAm\")[0])).group(1)\n",
    "                except : short_info = \"\"\n",
    "                    \n",
    "                try : long_info = str(soup.select(\"p._BAm\")[0]).split(\">\")[1].split(\"<\")[0]\n",
    "                except : long_info = \"\"\n",
    "                \n",
    "                #위키피디아 검색\n",
    "                \n",
    "                driver.find_elements_by_class_name(\"_Qzm\")[idx].click()\n",
    "                time.sleep(10)\n",
    "                \n",
    "                try :\n",
    "                    driver.find_element_by_class_name(\"q\").click()\n",
    "                    time.sleep(10)\n",
    "                except :\n",
    "                    time.sleep(10)\n",
    "                    \n",
    "                    try :\n",
    "                        driver.find_element_by_class_name(\"q\").click()\n",
    "                        time.sleep(5)\n",
    "                    except :\n",
    "                        temp_html = driver.page_source\n",
    "                        temp_soup = BeautifulSoup(temp_html, \"html.parser\")\n",
    "                        contents = temp_soup.select(\"body > div > div > div > div > div > div > div > div > div > div > div > div > div > div > div > div > div > span\")\n",
    "                    \n",
    "                        result.append((name, (nature, city, star, review, short_info, long_info, contents)))\n",
    "                        total += 1\n",
    "                    \n",
    "                        print(\"   \", name,\"err_no_wiki\")\n",
    "                        err_lst.append((nature, city))\n",
    "                        continue\n",
    "                \n",
    "                wiki_html = driver.page_source\n",
    "                soup2 = BeautifulSoup(wiki_html, \"html.parser\")\n",
    "                try :\n",
    "                    contents = soup2.select(\"body > div > div > div > div > p\")\n",
    "                except :\n",
    "                    print(\"   \",\"err_wiki\")\n",
    "                    err_lst.append((nature, city))\n",
    "                    contents = []\n",
    "    \n",
    "                driver.back()\n",
    "                time.sleep(3)\n",
    "            \n",
    "                result.append((name, (nature, city, star, review, short_info, long_info, contents)))\n",
    "                total += 1\n",
    "                \n",
    "            print(\"***\",total, \"clear\")\n",
    "        except :\n",
    "            err_lst.append((nature, city))\n",
    "            print(\"   \",\"err_fin\")\n",
    "        \n",
    "\n",
    "        tot+=1\n",
    "    \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_google_info = pd.DataFrame.from_items(result).T\n",
    "df_city_google_info.to_csv('travel_google_en2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
