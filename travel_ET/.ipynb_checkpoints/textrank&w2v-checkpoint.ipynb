{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import wikipedia\n",
    "import math\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup  \n",
    "from urllib.request import urlopen\n",
    "\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import gensim\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.api as sms\n",
    "import sklearn as sk\n",
    "import hdbscan\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pylab as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_color_codes()\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"travel_wiki_en_nlp1.csv\", encoding = \"cp949\")\n",
    "df = df.rename(columns = {\"Unnamed: 0\" : \"place\", \"0\" : \"nature\", \"1\" : \"city\", \"2\" : \"star\", \"3\" : \"review\", \"4\" : \"short_info\", \"5\": \"long_info\", \"6\" : \"f_wiki\", \"7\" : \"s_wiki\"})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTokenizer() :\n",
    "    def __init__(self, text) :\n",
    "        self.text = text\n",
    "        self.sentence = []\n",
    "        self.noun = []\n",
    "        self.word_count = []\n",
    "        \n",
    "    def making_sentence(self) :\n",
    "        self.text = self.text.replace(\"?\", \"\").replace(\"=\", \"\")\n",
    "        self.sentence = nltk.sent_tokenize(self.text)\n",
    "    \n",
    "    def making_noun(self) :\n",
    "        for sentence in self.sentence :\n",
    "            if sentence not in [\" \", \"\"] :\n",
    "                sentence = sentence.replace(\"'\", \"\")\n",
    "                tokens = nltk.word_tokenize(sentence)\n",
    "                tagged = nltk.pos_tag(tokens)\n",
    "                nn_tagged = list(filter(lambda x: \"NN\" in x[1], tagged))\n",
    "                nn_tagged = list(map(lambda x: x[0], nn_tagged))\n",
    "                self.noun.append(\" \".join(nn_tagged))\n",
    "                \n",
    "    def word_counter(self) :\n",
    "        dic = {}\n",
    "        \n",
    "        for noun in self.noun :\n",
    "            noun_lst = noun.split()\n",
    "            \n",
    "            for n in noun_lst :\n",
    "                if n in dic :\n",
    "                    dic[n] += 1\n",
    "                else :\n",
    "                    dic[n] = 1\n",
    "        \n",
    "        self.word_count = sorted(dic.items(), reverse = True, key = lambda x : x[1])\n",
    "        \n",
    "    def get_sentence(self) :\n",
    "        return self.sentence\n",
    "\n",
    "    def get_noun(self) :\n",
    "        return self.noun\n",
    "    \n",
    "    def get_word_count(self) :\n",
    "        return self.word_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphMatrix() :\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.cnt_vec = CountVectorizer()\n",
    "        self.graph_sentence = []\n",
    "    \n",
    "    def build_sent_graph(self, sentence):\n",
    "        tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "        self.graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "        \n",
    "        return self.graph_sentence\n",
    "    \n",
    "    def build_words_graph(self, sentence):\n",
    "        cnt_vec_mat = normalize(self.cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "        vocab = self.cnt_vec.vocabulary_\n",
    "        \n",
    "        return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rank(object):\n",
    "    def get_ranks(self, graph, d=0.85): # d = damping factor\n",
    "        A = graph\n",
    "        matrix_size = A.shape[0]\n",
    "        \n",
    "        for id in range(matrix_size):\n",
    "            A[id, id] = 0 # diagonal 부분을 0으로\n",
    "            link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\n",
    "            \n",
    "            if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "                A[:, id] *= -d\n",
    "                A[id, id] = 1\n",
    "        B = (1-d) * np.ones((matrix_size, 1))\n",
    "        \n",
    "        new_A = A+0.00001*np.random.rand(len(A), len(A)) # noise 발생\n",
    "        ranks = np.linalg.solve(new_A, B) # 연립방정식 Ax = b\n",
    "        \n",
    "        return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRank(object):\n",
    "    def __init__(self, text):\n",
    "        self.text = SentenceTokenizer(text)\n",
    "        self.text.making_sentence()\n",
    "        self.text.making_noun()\n",
    "        self.text.word_counter()\n",
    "            \n",
    "        self.sentences = self.text.get_sentence()\n",
    "        self.nouns = self.text.get_noun()\n",
    "        self.counts = self.text.get_word_count()\n",
    "        \n",
    "        self.graph_matrix = GraphMatrix()\n",
    "        self.sent_graph = self.graph_matrix.build_sent_graph(self.nouns)\n",
    "        self.words_graph, self.idx2word = self.graph_matrix.build_words_graph(self.nouns)\n",
    "        \n",
    "        self.rank = Rank()\n",
    "        self.sent_rank_idx = self.rank.get_ranks(self.sent_graph)\n",
    "        self.sorted_sent_rank_idx = sorted(self.sent_rank_idx, key=lambda k: self.sent_rank_idx[k], reverse=True)\n",
    "        \n",
    "        self.word_rank_idx = self.rank.get_ranks(self.words_graph)\n",
    "        self.sorted_word_rank_idx = sorted(self.word_rank_idx, key=lambda k: self.word_rank_idx[k], reverse=True)\n",
    "        \n",
    "    def summarize(self, sent_num=3):\n",
    "        summary = []\n",
    "        index=[]\n",
    "        \n",
    "        if sent_num > len(self.sorted_sent_rank_idx) :\n",
    "            sent_num = len(self.sorted_sent_rank_idx)\n",
    "        \n",
    "        for idx in self.sorted_sent_rank_idx[:sent_num]:\n",
    "            index.append(idx)\n",
    "        index.sort()\n",
    "        \n",
    "        for idx in index:\n",
    "            summary.append(self.sentences[idx])\n",
    "            \n",
    "        return summary\n",
    "    \n",
    "    def keywords(self, word_num=20):\n",
    "        \n",
    "        if word_num > len(self.sorted_word_rank_idx) :\n",
    "            word_num = len(self.sorted_word_rank_idx)\n",
    "        \n",
    "        rank = Rank()\n",
    "        rank_idx = rank.get_ranks(self.words_graph)\n",
    "        sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\n",
    "        keywords = []\n",
    "        index=[]\n",
    "        \n",
    "        for idx in sorted_rank_idx[:word_num]:\n",
    "            index.append(idx)\n",
    "            \n",
    "        #index.sort()\n",
    "        for idx in index:\n",
    "            keywords.append((self.idx2word[idx], rank_idx[idx]))\n",
    "            \n",
    "        return keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def making_sentence(text) :\n",
    "        result = []\n",
    "        \n",
    "        if type(text) != str or len(text) == 0 :\n",
    "            return []\n",
    "        \n",
    "        text = text.replace(\"?\", \"\").replace(\"=\", \"\")\n",
    "        sentence = nltk.sent_tokenize(text)\n",
    "        \n",
    "        for sent in sentence :\n",
    "            sent = [x.strip().lower() for x in re.split(r'([&~\"\\'\\(.;,\\) ])', sent) if x not in [\" \", \"\"]] \n",
    "            result.append(sent)\n",
    "               \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v(place, s_idx, w_idx) :\n",
    "    result = []\n",
    "    \n",
    "    try :\n",
    "        sentences = place[1][s_idx]\n",
    "        words =  place[1][w_idx]\n",
    "        model = word2vec.Word2Vec(sentences)\n",
    "\n",
    "        for idx in words :\n",
    "            try :\n",
    "                vec = model.wv[idx[0]]\n",
    "                result.append((idx[0], vec))\n",
    "            except :\n",
    "                continue\n",
    "\n",
    "        return result\n",
    "    \n",
    "    except :\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v2(place, s_idx, w_idx, model) :\n",
    "    result = []\n",
    "    \n",
    "    try :\n",
    "        sentences = place[1][s_idx]\n",
    "        words =  place[1][w_idx]\n",
    "\n",
    "        for idx in words :\n",
    "            try :\n",
    "                vec = model.wv[idx[0]]\n",
    "                result.append((idx[0], vec))\n",
    "            except :\n",
    "                continue\n",
    "\n",
    "        return result\n",
    "    \n",
    "    except :\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_vector(idx, vector) :\n",
    "    return vector[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist (A,B):\n",
    "    return (B[0], np.linalg.norm(np.asarray(A[1])-np.asarray(B[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist2 (A,B):\n",
    "    return np.dot(A[1], B[1]) / (math.sqrt(np.dot(A[1],A[1])) * math.sqrt(np.dot(B[1],B[1]))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"f_split\"] = df.f_wiki.map(lambda x : making_sentence(x))\n",
    "df[\"s_split\"] = df.s_wiki.map(lambda x : making_sentence(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for idx in range(len(df)):\n",
    "    temp = df.iloc[idx]\n",
    "    result.append((temp[0],(temp[1], temp[2], temp[3], temp[4], temp[5], temp[6], temp[7], temp[8], temp[9], temp[10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "err_lst = []\n",
    "result2 = []\n",
    "\n",
    "for idx, place in enumerate(result) :\n",
    "    if place[1][6] in (\"\", \" \")  : \n",
    "        continue\n",
    "    \n",
    "    try :\n",
    "        if place[1][7] in (\"\", \" \") :\n",
    "            continue\n",
    "            \n",
    "        else :\n",
    "            f_textrank = TextRank(place[1][6])\n",
    "            s_textrank = TextRank(place[1][7])   \n",
    "\n",
    "            f_keyword = f_textrank.keywords()\n",
    "            s_keyword = s_textrank.keywords()\n",
    "\n",
    "            result2.append((place[0], (place[1][0], place[1][1], place[1][2], place[1][3], place[1][4], place[1][5], place[1][6], place[1][7], place[1][8], place[1][9], f_keyword, s_keyword)))    \n",
    "\n",
    "    except :\n",
    "        err_lst.append((place[0], (place[1][0], place[1][1])))\n",
    "        print(place[0],\"err\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_w2v = pd.DataFrame.from_items(result2).T\n",
    "\n",
    "print(len(wiki_w2v))\n",
    "wiki_w2v.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3 = []\n",
    "\n",
    "for place in result2 :\n",
    "    result3.append((place[0], (place[1][0],place[1][1], place[1][2], place[1][3], w2v(place, 8, 10), w2v(place, 9, 11), place[1][10], place[1][11])))\n",
    "    \n",
    "wiki_w2v_tfidf = pd.DataFrame.from_items(result3).T\n",
    "\n",
    "print(len(wiki_w2v_tfidf))\n",
    "wiki_w2v_tfidf = wiki_w2v_tfidf.rename(columns = {0 : \"nature\", 1 : \"city\", 2 : \"star\", 3 : \"review\", 4 : \"f_w2v\", 5 : \"s_w2v\", 6 : \"f_tfidf\", 7 : \"s_tfidf\"})\n",
    "wiki_w2v_tfidf.head(30)\n",
    "\n",
    "wiki_w2v_tfidf = wiki_w2v_tfidf.reset_index().rename(columns = {\"index\" : \"place\"})\n",
    "wiki_w2v_tfidf.head(10).f_w2v[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wiki_w2v_tfidf.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vector = []\n",
    "\n",
    "\n",
    "for idx in range(len(wiki_w2v_tfidf)) :\n",
    "    V = np.asarray([float(0)]*100)\n",
    "    \n",
    "    w2v = wiki_w2v_tfidf.iloc[idx][5]\n",
    "    tfidf = dict(wiki_w2v_tfidf.iloc[idx][7])\n",
    "    \n",
    "    for w in w2v :\n",
    "        if  tfidf[w[0]] > 0 :\n",
    "            #constant = wiki_w2v.iloc[idx][6].count(w[0])\n",
    "            constant = 1\n",
    "            v = constant * tfidf[w[0]] * np.asarray(w[1])\n",
    "            V += v\n",
    "    \n",
    "    vector.append(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wiki_w2v_tfidf[\"vector\"] = 0\n",
    "wiki_w2v_tfidf[\"vector\"] = wiki_w2v_tfidf.index.map(lambda x : making_vector(x, vector))\n",
    "wiki_w2v_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_w2v_tfidf2 = wiki_w2v_tfidf.drop([\"s_w2v\", \"s_tfidf\"], axis = 1)\n",
    "print(len(wiki_w2v_tfidf2))\n",
    "wiki_w2v_tfidf2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector2 = list(map(lambda x: (0, x.tolist()), vector))\n",
    "\n",
    "vector_df = pd.DataFrame.from_items(vector2).T\n",
    "vector_df = vector_df.reset_index().drop(\"index\", axis =1)\n",
    "vector_df.index.names = [\"index\"]\n",
    "vector_df = vector_df.reset_index()\n",
    "\n",
    "print(len(vector_df.columns))\n",
    "vector_df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = [\"index\"]\n",
    "feature = list(range(100))\n",
    "\n",
    "cluster = hdbscan.HDBSCAN()\n",
    "cluster.fit(vector_df[feature], vector_df[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = {}\n",
    "\n",
    "for idx in list(cluster.labels_) :\n",
    "    if idx in label :\n",
    "        label[idx] += 1\n",
    "    else :\n",
    "        label[idx] = 1\n",
    "\n",
    "label_lst = sorted(label.items(), reverse = True, key = lambda x : x[1])\n",
    "label_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
