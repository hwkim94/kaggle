{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 처리(요약)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import wikipedia\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup  \n",
    "from urllib.request import urlopen\n",
    "\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 데이터 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#셀리옴으로 크롤링하기\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "options.add_argument('window-size=1920x1080')\n",
    "#options.add_argument(\"disable-gpu\")\n",
    "options.add_argument(\"--disable-gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('C:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\selenium\\\\webdriver\\\\chrome\\\\chromedriver.exe', chrome_options=options)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#나라별 도시 크롤링\n",
    "country =  [\"스페인\", \"포르투갈\"]\n",
    "dic = {}\n",
    "for idx in country :\n",
    "    dic[idx] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "for nature in country :\n",
    "    base_url = 'https://www.google.co.kr/search?' +'q=' + nature + '+도시&hl=en'\n",
    "    driver.get(base_url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    lst = driver.find_elements_by_css_selector(\"a.klitem\")\n",
    "    for city in lst :\n",
    "        dic[nature].append(city.text.split(\"\\n\")[0])\n",
    "        tot += 1\n",
    "        \n",
    "print(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "err_lst = []\n",
    "\n",
    "regex1 = \">(.*)<\"\n",
    "regex2 = \"Rated (.*) out\"\n",
    "regex3 = \"\\((.*)\\)\"\n",
    "regex4 = \">(.*)<\"\n",
    "regex5 = \"\"\n",
    "\n",
    "tot = 1\n",
    "a = 0\n",
    "total = 0\n",
    "for nature in country :\n",
    "    for city in dic[nature] :\n",
    "        print(tot, nature, city, end = \" \")\n",
    "        a+=1\n",
    "        \n",
    "        try : \n",
    "            #hl=en 을 넣어주면 영어 검색\n",
    "            base_url = 'https://www.google.co.kr/search?' +'q=' + nature + '+' + city + '+' + \"관광지&hl=en\"\n",
    "            driver.get(base_url)\n",
    "            time.sleep(5)\n",
    "            \n",
    "            #mor_buttin_id가 수시로 바뀌니 체크하기\n",
    "            more_button_id = '_x7n'\n",
    "            driver.find_element_by_class_name(more_button_id).click()\n",
    "            time.sleep(5)\n",
    "\n",
    "            html =  driver.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            temp = soup.select(\"body > div > div > div > ol > li > div > div\")\n",
    "            \n",
    "            total = 0\n",
    "            for idx, place in enumerate(temp) :\n",
    "                soup = BeautifulSoup(str(place), \"html.parser\")\n",
    "                \n",
    "                name = re.search(regex1, str(soup.select(\"div > h2\")[0])).group(1)\n",
    "                \n",
    "                try : star = float(re.search(regex2, str(soup.select(\"div >  g-review-stars > span\")[0])).group(1))\n",
    "                except : star = 0\n",
    "                    \n",
    "                try : review = int(re.search(regex3, str(soup.select(\"div > div  > span._Mnc\")[0])).group(1).replace(\",\", \"\"))\n",
    "                except : review = 0\n",
    "                \n",
    "                try : short_info = re.search(regex4, str(soup.select(\"p._DAm\")[0])).group(1)\n",
    "                except : short_info = \"\"\n",
    "                    \n",
    "                try : long_info = str(soup.select(\"p._BAm\")[0]).split(\">\")[1].split(\"<\")[0]\n",
    "                except : long_info = \"\"\n",
    "                \n",
    "                #위키피디아 검색\n",
    "                \n",
    "                try :\n",
    "                    page = wikipedia.page(name)\n",
    "                    time.sleep(5)\n",
    "                    summary = page.summary\n",
    "                    content = page.content\n",
    "                    time.sleep(5)\n",
    "                except :\n",
    "                    summary = \"\"\n",
    "                    content = \"\" \n",
    "                \n",
    "                result.append((name, (nature, city, star, review, short_info, long_info, content, summary)))\n",
    "                total += 1\n",
    "                    \n",
    "                \n",
    "            print(\"***\",total, \"clear\")\n",
    "        except :\n",
    "            err_lst.append((nature, city))\n",
    "            print(\"   \",\"err_fin\")\n",
    "        \n",
    "\n",
    "        tot+=1\n",
    "    \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df = pd.DataFrame.from_items(result).T\n",
    "wiki_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df.to_csv(\"travel_wiki_en_nlp1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF를 통한 문장요약&키워드 추출 및 WORD COUNT\n",
    "\n",
    "http://excelsior-cjh.tistory.com/entry/TextRank%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EB%AC%B8%EC%84%9C%EC%9A%94%EC%95%BD\n",
    "\n",
    "tf : 문장 내에서 특정 단어의 빈도수 \n",
    "\n",
    "idf : 전체 문장의 수/ 특정 단어가 나오는 문장의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"travel_wiki_en_nlp1.csv\", encoding = \"cp949\")\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for idx in range(len(df)):\n",
    "    temp = df.iloc[idx]\n",
    "    result.append((temp[0],(temp[1], temp[2], temp[3], temp[4], temp[5], temp[6], temp[7], temp[8])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTokenizer() :\n",
    "    def __init__(self, text) :\n",
    "        self.text = text\n",
    "        self.sentence = []\n",
    "        self.noun = []\n",
    "        self.word_count = []\n",
    "        \n",
    "    def making_sentence(self) :\n",
    "        self.sentence = nltk.sent_tokenize(self.text)\n",
    "        self.sentence = list(filter(lambda x: \"=\" not in x, self.sentence))\n",
    "    \n",
    "    def making_noun(self) :\n",
    "        for sentence in self.sentence :\n",
    "            if sentence != \" \" :\n",
    "                tokens = nltk.word_tokenize(sentence)\n",
    "                tagged = nltk.pos_tag(tokens)\n",
    "                nn_tagged = list(filter(lambda x: \"NN\" in x[1], tagged))\n",
    "                nn_tagged = list(map(lambda x: x[0], nn_tagged))\n",
    "                self.noun.append(\" \".join(nn_tagged))\n",
    "                \n",
    "    def word_counter(self) :\n",
    "        dic = {}\n",
    "        \n",
    "        for noun in self.noun :\n",
    "            noun_lst = noun.split()\n",
    "            \n",
    "            for n in noun_lst :\n",
    "                if n in dic :\n",
    "                    dic[n] += 1\n",
    "                else :\n",
    "                    dic[n] = 1\n",
    "        \n",
    "        self.word_count = sorted(dic.items(), reverse = True, key = lambda x : x[1])\n",
    "        \n",
    "    def get_sentence(self) :\n",
    "        return self.sentence\n",
    "\n",
    "    def get_noun(self) :\n",
    "        return self.noun\n",
    "    \n",
    "    def get_word_count(self) :\n",
    "        return self.word_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphMatrix() :\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.cnt_vec = CountVectorizer()\n",
    "        self.graph_sentence = []\n",
    "    \n",
    "    def build_sent_graph(self, sentence):\n",
    "        tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "        self.graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "        \n",
    "        return self.graph_sentence\n",
    "    \n",
    "    def build_words_graph(self, sentence):\n",
    "        cnt_vec_mat = normalize(self.cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "        vocab = self.cnt_vec.vocabulary_\n",
    "        \n",
    "        return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rank(object):\n",
    "    def get_ranks(self, graph, d=0.85): # d = damping factor\n",
    "        A = graph\n",
    "        matrix_size = A.shape[0]\n",
    "        \n",
    "        for id in range(matrix_size):\n",
    "            A[id, id] = 0 # diagonal 부분을 0으로\n",
    "            link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\n",
    "            \n",
    "            if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "                A[:, id] *= -d\n",
    "                A[id, id] = 1\n",
    "        B = (1-d) * np.ones((matrix_size, 1))\n",
    "        \n",
    "        new_A = A+0.00001*np.random.rand(len(A), len(A)) # noise 발생\n",
    "        ranks = np.linalg.solve(new_A, B) # 연립방정식 Ax = b\n",
    "        \n",
    "        return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRank(object):\n",
    "    def __init__(self, text):\n",
    "        self.text = SentenceTokenizer(text)\n",
    "        self.text.making_sentence()\n",
    "        self.text.making_noun()\n",
    "        self.text.word_counter()\n",
    "            \n",
    "        self.sentences = self.text.get_sentence()\n",
    "        self.nouns = self.text.get_noun()\n",
    "        self.counts = self.text.get_word_count()\n",
    "        \n",
    "        self.graph_matrix = GraphMatrix()\n",
    "        self.sent_graph = self.graph_matrix.build_sent_graph(self.nouns)\n",
    "        self.words_graph, self.idx2word = self.graph_matrix.build_words_graph(self.nouns)\n",
    "        \n",
    "        self.rank = Rank()\n",
    "        self.sent_rank_idx = self.rank.get_ranks(self.sent_graph)\n",
    "        self.sorted_sent_rank_idx = sorted(self.sent_rank_idx, key=lambda k: self.sent_rank_idx[k], reverse=True)\n",
    "        \n",
    "        self.word_rank_idx = self.rank.get_ranks(self.words_graph)\n",
    "        self.sorted_word_rank_idx = sorted(self.word_rank_idx, key=lambda k: self.word_rank_idx[k], reverse=True)\n",
    "        \n",
    "    def summarize(self, sent_num=3):\n",
    "        summary = []\n",
    "        index=[]\n",
    "        \n",
    "        if sent_num > len(self.sorted_sent_rank_idx) :\n",
    "            sent_num = len(self.sorted_sent_rank_idx)\n",
    "        \n",
    "        for idx in self.sorted_sent_rank_idx[:sent_num]:\n",
    "            index.append(idx)\n",
    "        index.sort()\n",
    "        \n",
    "        for idx in index:\n",
    "            summary.append(self.sentences[idx])\n",
    "            \n",
    "        return summary\n",
    "    \n",
    "    def keywords(self, word_num=10):\n",
    "        \n",
    "        if word_num > len(self.sorted_word_rank_idx) :\n",
    "            word_num = len(self.sorted_word_rank_idx)\n",
    "        \n",
    "        rank = Rank()\n",
    "        rank_idx = rank.get_ranks(self.words_graph)\n",
    "        sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\n",
    "        keywords = []\n",
    "        index=[]\n",
    "        \n",
    "        for idx in sorted_rank_idx[:word_num]:\n",
    "            index.append(idx)\n",
    "            \n",
    "        #index.sort()\n",
    "        for idx in index:\n",
    "            keywords.append(self.idx2word[idx])\n",
    "            \n",
    "        return keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_lst = []\n",
    "\n",
    "for idx, place in enumerate(result) :\n",
    "    if place[1][6] in (\"\", \" \")  : \n",
    "        continue\n",
    "    \n",
    "    try :\n",
    "        if place[1][7] not in (\"\", \" \") :\n",
    "            f_textrank = TextRank(place[1][6])\n",
    "            s_textrank = TextRank(place[1][7])   \n",
    "\n",
    "            f_summary = f_textrank.summarize()\n",
    "            s_summary = s_textrank.summarize()\n",
    "            f_keyword = f_textrank.keywords()\n",
    "            s_keyword = s_textrank.keywords()\n",
    "\n",
    "            result[idx] = (place[0], (place[1][0], place[1][1], place[1][2], place[1][3], place[1][4], place[1][5], place[1][6], place[1][7], f_summary, s_summary, f_keyword, s_keyword, f_textrank.counts, s_textrank.counts))\n",
    "\n",
    "        else :\n",
    "\n",
    "            f_textrank = TextRank(place[1][6])\n",
    "            s_textrank = []\n",
    "\n",
    "            f_summary = f_textrank.summarize()\n",
    "            s_summary = []\n",
    "            f_keyword = f_textrank.keywords()\n",
    "            s_keyword = []\n",
    "\n",
    "            result[idx] = (place[0], (place[1][0], place[1][1], place[1][2], place[1][3], place[1][4], place[1][5], place[1][6], place[1][7], [] ,[], [],[], 0,0))\n",
    "    except :\n",
    "        err_lst.append((place[0], (place[1][0], place[1][1])))\n",
    "        print(place[0],\"err\")\n",
    "        result[idx] = (place[0], (place[1][0], place[1][1], place[1][2], place[1][3], place[1][4], place[1][5], place[1][6], place[1][7], [] ,[], [],[], 0,0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wiki_tfidf = pd.DataFrame.from_items(result).T\n",
    "wiki_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tfidf.to_csv(\"travel_wiki_tfidf1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
