{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br> <br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN() :\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        \n",
    "    def convolution(self, X_input, filters, kernel_size, strides, name, padding=\"SAME\") :\n",
    "        with tf.variable_scope(name) :\n",
    "            bn = tf.layers.batch_normalization(X_input)\n",
    "            conv = tf.layers.conv2d(bn, filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            relu = tf.nn.leaky_relu(conv)\n",
    "            \n",
    "            return relu\n",
    "            \n",
    "    def build(self) :\n",
    "        with tf.variable_scope(self.name) :\n",
    "            ### Input\n",
    "            #input : 128x126x1\n",
    "            #output : 8\n",
    "            self.X = tf.placeholder(tf.float32, [None, 128, 126, 1])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, 8])\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "            self.learning_rate = tf.placeholder(tf.float32)\n",
    "            print(self.X.shape)\n",
    "            \n",
    "        ### Input Layer\n",
    "        #input : 128x126x1\n",
    "        #output : 32x31x8\n",
    "        conv1 = self.convolution(self.X, 8, [3,3], 2, \"conv1\")\n",
    "        pool1 = tf.layers.max_pooling2d(conv1, pool_size=[2,2], strides=2, name=\"pool1\")\n",
    "        print(conv1.shape)\n",
    "        print(pool1.shape)\n",
    "\n",
    "        ### Hidden Layer1\n",
    "        #input : 32x31x8\n",
    "        #output : 32x31x16\n",
    "        conv2 = self.convolution(conv1, 16, [3,3], 1, \"conv2\")\n",
    "        print(conv2.shape)\n",
    "            \n",
    "        ### Hidden Layer2\n",
    "        #input : 32x31x16\n",
    "        #output : 32x31x32\n",
    "        conv3 = self.convolution(conv2, 32, [3,3], 1, \"conv3\")\n",
    "        print(conv3.shape)\n",
    "            \n",
    "        ### Pooling Layer2\n",
    "        #input : 32x31x32\n",
    "        #output : 16x15x32\n",
    "        pool2 = tf.layers.max_pooling2d(conv3, pool_size=[2,2], strides=2, name=\"pool2\")\n",
    "        print(pool2.shape)\n",
    "            \n",
    "        ### Hidden Layer3\n",
    "        #input : 16x15x32\n",
    "        #output : 16x15x64\n",
    "        conv4 = self.convolution(pool2, 64, [3,3], 1, \"conv4\")\n",
    "        print(conv4.shape)\n",
    "        \n",
    "        ### Hidden Layer4\n",
    "        #input : 16x15x64\n",
    "        #output : 16x15x128\n",
    "        conv5 = self.convolution(conv4, 128, [3,3], 1, \"conv5\")\n",
    "        print(conv5.shape)\n",
    "        \n",
    "        ### Pooling Layer3\n",
    "        #input : 16x15x128\n",
    "        #output : 8x7x128\n",
    "        pool3 = tf.layers.max_pooling2d(conv5, pool_size=[2,2], strides=2, name=\"pool3\")\n",
    "        print(pool3.shape)\n",
    "        \n",
    "        ### Hidden Layer5\n",
    "        #input : 8x7x128\n",
    "        #output : 8x7x32\n",
    "        conv6 = self.convolution(pool3, 32, [1,1], 1, \"conv6\")\n",
    "        print(conv6.shape)\n",
    "        \n",
    "        with tf.variable_scope(\"global_avg_pooling\") :\n",
    "            ### global avg pooling\n",
    "            #input : 8x7x32\n",
    "            #output : 1x1x32\n",
    "            global_avg_pooling = tf.reduce_mean(conv6, [1, 2], keep_dims=True)\n",
    "            print(global_avg_pooling.shape)\n",
    "        \n",
    "        with tf.variable_scope(\"fully_connected\") :\n",
    "            ###Output Layer\n",
    "            #input : 1x1x32\n",
    "            #ouput : 8\n",
    "            shape = global_avg_pooling.get_shape().as_list()\n",
    "            dimension = shape[1] * shape[2] * shape[3]\n",
    "            flat = tf.reshape(global_avg_pooling, shape=[-1, dimension])\n",
    "\n",
    "            fc = tf.layers.dense(inputs=flat, units=8, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.logits = fc\n",
    "\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.Y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))     \n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, x_test, training=False):\n",
    "        feed_dict={self.X: x_test, self.training: training}\n",
    "        \n",
    "        return self.sess.run(self.logits, feed_dict=feed_dict)\n",
    "\n",
    "    def get_accuracy(self, x_test, y_test, training=False):\n",
    "        feed_dict={self.X: x_test,self.Y: y_test, self.training: training}\n",
    "        \n",
    "        return self.sess.run(self.accuracy, feed_dict=feed_dict)\n",
    "\n",
    "    def train(self, x_data, y_data, learning_rate, training=True):\n",
    "        feed_dict={self.X: x_data, self.Y: y_data, self.learning_rate: learning_rate, self.training: training}\n",
    "        \n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict=feed_dict)\n",
    "    \n",
    "    def evaluate(self, X_input, Y_input, batch_size=None, training=False):\n",
    "        N = X_input.shape[0]\n",
    "            \n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "            \n",
    "        for i in range(0, N, batch_size):\n",
    "            X_batch = X_input[i:i + batch_size]\n",
    "            Y_batch = Y_input[i:i + batch_size]\n",
    "                \n",
    "            feed_dict = {self.X: X_batch, self.Y: Y_batch, self.training: training}\n",
    "                \n",
    "            loss = self.cost\n",
    "            accuracy = self.accuracy\n",
    "                \n",
    "            step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed_dict)\n",
    "                \n",
    "            total_loss += step_loss * X_batch.shape[0]\n",
    "            total_acc += step_acc * X_batch.shape[0]\n",
    "            \n",
    "        total_loss /= N\n",
    "        total_acc /= N\n",
    "            \n",
    "        return total_loss, total_acc\n",
    "    \n",
    "    def save(self, ver) :\n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(self.sess, \"CNN_\" + str(ver) + \".ckpt\")\n",
    "        \n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br> <br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav_data(path, num =150) :\n",
    "    file_lst = os.listdir(path)\n",
    "    random.shuffle(file_lst)\n",
    "    \n",
    "    file_lst = file_lst[:num]\n",
    "    \n",
    "    train = []\n",
    "    valid = []\n",
    "    test = []\n",
    "    all_data = []\n",
    "    \n",
    "    for file in file_lst :\n",
    "        try : \n",
    "            y, sr = librosa.load(path+file)\n",
    "            emotion = int(file.split(\"-\")[2])\n",
    "            actor = int(file.split(\"-\")[6].split(\".\")[0])\n",
    "        \n",
    "            melspectrogram = librosa.feature.melspectrogram(y, sr=sr, n_mels=128)\n",
    "        \n",
    "            if actor in [1,2] :\n",
    "                valid.append((melspectrogram, emotion))\n",
    "            elif actor in [3,4] :\n",
    "                test.append((melspectrogram, emotion))\n",
    "            else :\n",
    "                train.append((melspectrogram, emotion))\n",
    "            \n",
    "            all_data.append((melspectrogram, emotion))\n",
    "        except :\n",
    "            pass\n",
    "    \n",
    "    return file_lst, train, valid, test, all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutting(train, valid, test, all_data, size=1025, num=276) :\n",
    "    result = []\n",
    "    half = int(num/2)\n",
    "    \n",
    "    for dataset in [train, valid, test, all_data] :\n",
    "        zero = np.zeros([len(dataset), size, num])\n",
    "        emotion_lst = []\n",
    "\n",
    "        idx = 0\n",
    "        for spectrogram, emotion in dataset:\n",
    "            mid = int(spectrogram.shape[1]/2)\n",
    "            zero[idx, :, 0:len(spectrogram[0])] = spectrogram[:, mid-half:mid+half]\n",
    "            emotion_lst.append(emotion-1)\n",
    "            idx += 1\n",
    "            \n",
    "        result.append((zero, emotion_lst))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encoding(data, num=8) :\n",
    "    return np.eye(num)[data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_pair(name, x) :\n",
    "    return pyspark.sql.Row(file = name,\n",
    "                           neutral = round(float(x[0]),2),\n",
    "                           calm = round(float(x[1]),2),\n",
    "                           happy = round(float(x[2]),2),\n",
    "                           sad = round(float(x[3]),2),\n",
    "                           angry = round(float(x[4]),2),\n",
    "                           fearful = round(float(x[5]),2),\n",
    "                           disgust = round(float(x[6]),2),\n",
    "                           surprised = round(float(x[7]),2))\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br> <br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = sys.argv[1]\n",
    "data_dir = sys.argv[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br> <br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melspectrogram from wav\n",
    "file_lst, train, valid, test, all_data = load_wav_data(data_dir)\n",
    "cut_train, cut_valid, cut_test, cut_all = cutting(train, valid, test, all_data, size =128 , num=126)\n",
    "\n",
    "all_data = cut_all[0].reshape([-1, 128, 126, 1])\n",
    "all_label = onehot_encoding(cut_all[1])\n",
    "\n",
    "train = []\n",
    "valid = []\n",
    "test = []\n",
    "cut_train = []\n",
    "cut_valid = []\n",
    "cut_test = []\n",
    "cut_all = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 128, 126, 1)\n",
      "(?, 64, 63, 8)\n",
      "(?, 32, 31, 8)\n",
      "(?, 64, 63, 16)\n",
      "(?, 64, 63, 32)\n",
      "(?, 32, 31, 32)\n",
      "(?, 32, 31, 64)\n",
      "(?, 32, 31, 128)\n",
      "(?, 16, 15, 128)\n",
      "(?, 16, 15, 32)\n",
      "WARNING:tensorflow:From <ipython-input-10-282b193006c9>:79: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "(?, 1, 1, 32)\n",
      "WARNING:tensorflow:From <ipython-input-10-282b193006c9>:93: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "model = CNN(sess, \"CNN\")\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./CNN/CNN_model/ver_2/CNN_2.ckpt\n"
     ]
    }
   ],
   "source": [
    "ver = 2\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66\n"
     ]
    }
   ],
   "source": [
    "print(model.get_accuracy(all_data, all_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sess.run([tf.nn.softmax(model.logits)], feed_dict={model.X : all_data, model.Y : all_label, model.learning_rate:0.01, model.training:False})\n",
    "result_array = np.array(result).reshape([-1, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_rdd = sc.parallelize(file_lst)\n",
    "result_rdd = sc.parallelize(result_array)\n",
    "rdd = file_rdd.zip(result_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rdd2 = rdd.map(lambda x: making_pair(x[0], list(np.round(x[1]*100, 2))))\n",
    "result_lst = rdd2.collect()\n",
    "result_df = spark.createDataFrame(result_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+-----+-----+-----+-------+-------+---------+\n",
      "|                file|neutral| calm|happy|  sad|angry|fearful|disgust|surprised|\n",
      "+--------------------+-------+-----+-----+-----+-----+-------+-------+---------+\n",
      "|03-01-08-02-02-01...|    0.0|  0.0| 0.09|  0.0| 0.01|    0.0|    0.0|     99.9|\n",
      "|03-01-03-02-01-02...|    0.0|  0.0|  0.0|  0.0|100.0|    0.0|    0.0|      0.0|\n",
      "|03-01-05-01-01-01...|    0.0|  0.0|34.53| 0.02|45.85|  17.15|   1.58|     0.87|\n",
      "|03-01-05-02-01-01...|    0.0|  0.0|  0.0|  0.0|100.0|    0.0|    0.0|      0.0|\n",
      "|03-01-07-01-01-01...|   2.65|  0.5|20.59|32.67| 2.43|  20.91|   4.97|    15.28|\n",
      "|03-01-06-01-02-02...|   0.03|  0.0|30.64|  2.9| 9.27|  46.88|   3.37|     6.92|\n",
      "|03-01-02-01-01-02...|  38.54|36.91| 1.91|18.57| 0.19|   1.43|   2.41|     0.03|\n",
      "|03-01-05-01-02-02...|    0.0|  0.0| 2.36|  0.0|96.95|   0.26|   0.43|      0.0|\n",
      "|03-01-05-01-01-02...|    0.0|  0.0|  0.0|  0.0|100.0|    0.0|    0.0|      0.0|\n",
      "|03-01-04-02-01-02...|    0.0|  0.0|  0.0|100.0|  0.0|    0.0|    0.0|      0.0|\n",
      "+--------------------+-------+-----+-----+-----+-----+-------+-------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df2 = result_df.select([\"file\", \"neutral\", \"calm\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\", \"surprised\"])\n",
    "result_df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df2.createOrReplaceTempView(\"result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table's name is 'result'\n",
      "query >>> exit\n"
     ]
    }
   ],
   "source": [
    "while True :\n",
    "    print(\"Table's name is 'result'\")\n",
    "    query = input(\"query >>> \")\n",
    "    \n",
    "    if query == \"exit\" :\n",
    "        break\n",
    "    \n",
    "    try :\n",
    "        spark.sql(query).show(30)\n",
    "    except :\n",
    "        print(\"Invalid SQL syntax\")\n",
    "        print(\"\\n\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
