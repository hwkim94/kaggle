{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkling Emoticana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN() :\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        \n",
    "    def convolution(self, X_input, filters, kernel_size, strides, name, padding=\"SAME\") :\n",
    "        with tf.variable_scope(name) :\n",
    "            bn = tf.layers.batch_normalization(X_input)\n",
    "            conv = tf.layers.conv2d(bn, filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            relu = tf.nn.leaky_relu(conv)\n",
    "            \n",
    "            return relu\n",
    "            \n",
    "    def build(self) :\n",
    "        with tf.variable_scope(self.name) :\n",
    "            ### Input\n",
    "            #input : 128x126x1\n",
    "            #output : 8\n",
    "            self.X = tf.placeholder(tf.float32, [None, 128, 126, 1])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, 8])\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "            self.learning_rate = tf.placeholder(tf.float32)\n",
    "            print(self.X.shape)\n",
    "            \n",
    "        ### Input Layer\n",
    "        #input : 128x126x1\n",
    "        #output : 32x31x8\n",
    "        conv1 = self.convolution(self.X, 8, [3,3], 2, \"conv1\")\n",
    "        pool1 = tf.layers.max_pooling2d(conv1, pool_size=[2,2], strides=2, name=\"pool1\")\n",
    "        print(conv1.shape)\n",
    "        print(pool1.shape)\n",
    "\n",
    "        ### Hidden Layer1\n",
    "        #input : 32x31x8\n",
    "        #output : 32x31x16\n",
    "        conv2 = self.convolution(conv1, 16, [3,3], 1, \"conv2\")\n",
    "        print(conv2.shape)\n",
    "            \n",
    "        ### Hidden Layer2\n",
    "        #input : 32x31x16\n",
    "        #output : 32x31x32\n",
    "        conv3 = self.convolution(conv2, 32, [3,3], 1, \"conv3\")\n",
    "        print(conv3.shape)\n",
    "            \n",
    "        ### Pooling Layer2\n",
    "        #input : 32x31x32\n",
    "        #output : 16x15x32\n",
    "        pool2 = tf.layers.max_pooling2d(conv3, pool_size=[2,2], strides=2, name=\"pool2\")\n",
    "        print(pool2.shape)\n",
    "            \n",
    "        ### Hidden Layer3\n",
    "        #input : 16x15x32\n",
    "        #output : 16x15x64\n",
    "        conv4 = self.convolution(pool2, 64, [3,3], 1, \"conv4\")\n",
    "        print(conv4.shape)\n",
    "        \n",
    "        ### Hidden Layer4\n",
    "        #input : 16x15x64\n",
    "        #output : 16x15x128\n",
    "        conv5 = self.convolution(conv4, 128, [3,3], 1, \"conv5\")\n",
    "        print(conv5.shape)\n",
    "        \n",
    "        ### Pooling Layer3\n",
    "        #input : 16x15x128\n",
    "        #output : 8x7x128\n",
    "        pool3 = tf.layers.max_pooling2d(conv5, pool_size=[2,2], strides=2, name=\"pool3\")\n",
    "        print(pool3.shape)\n",
    "        \n",
    "        ### Hidden Layer5\n",
    "        #input : 8x7x128\n",
    "        #output : 8x7x32\n",
    "        conv6 = self.convolution(pool3, 32, [1,1], 1, \"conv6\")\n",
    "        print(conv6.shape)\n",
    "        \n",
    "        with tf.variable_scope(\"global_avg_pooling\") :\n",
    "            ### global avg pooling\n",
    "            #input : 8x7x32\n",
    "            #output : 1x1x32\n",
    "            global_avg_pooling = tf.reduce_mean(conv6, [1, 2], keep_dims=True)\n",
    "            print(global_avg_pooling.shape)\n",
    "        \n",
    "        with tf.variable_scope(\"fully_connected\") :\n",
    "            ###Output Layer\n",
    "            #input : 1x1x32\n",
    "            #ouput : 8\n",
    "            shape = global_avg_pooling.get_shape().as_list()\n",
    "            dimension = shape[1] * shape[2] * shape[3]\n",
    "            flat = tf.reshape(global_avg_pooling, shape=[-1, dimension])\n",
    "\n",
    "            fc = tf.layers.dense(inputs=flat, units=8, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.logits = fc\n",
    "\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.Y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))     \n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, x_test, training=False):\n",
    "        feed_dict={self.X: x_test, self.training: training}\n",
    "        \n",
    "        return self.sess.run(self.logits, feed_dict=feed_dict)\n",
    "\n",
    "    def get_accuracy(self, x_test, y_test, training=False):\n",
    "        feed_dict={self.X: x_test,self.Y: y_test, self.training: training}\n",
    "        \n",
    "        return self.sess.run(self.accuracy, feed_dict=feed_dict)\n",
    "\n",
    "    def train(self, x_data, y_data, learning_rate, training=True):\n",
    "        feed_dict={self.X: x_data, self.Y: y_data, self.learning_rate: learning_rate, self.training: training}\n",
    "        \n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict=feed_dict)\n",
    "    \n",
    "    def evaluate(self, X_input, Y_input, batch_size=None, training=False):\n",
    "        N = X_input.shape[0]\n",
    "            \n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "            \n",
    "        for i in range(0, N, batch_size):\n",
    "            X_batch = X_input[i:i + batch_size]\n",
    "            Y_batch = Y_input[i:i + batch_size]\n",
    "                \n",
    "            feed_dict = {self.X: X_batch, self.Y: Y_batch, self.training: training}\n",
    "                \n",
    "            loss = self.cost\n",
    "            accuracy = self.accuracy\n",
    "                \n",
    "            step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed_dict)\n",
    "                \n",
    "            total_loss += step_loss * X_batch.shape[0]\n",
    "            total_acc += step_acc * X_batch.shape[0]\n",
    "            \n",
    "        total_loss /= N\n",
    "        total_acc /= N\n",
    "            \n",
    "        return total_loss, total_acc\n",
    "    \n",
    "    def save(self, ver) :\n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(self.sess, \"CNN_\" + str(ver) + \".ckpt\")\n",
    "        \n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <br></br>  <br></br>  <br></br>  <br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mat_data(path) :\n",
    "    file_lst = os.listdir(path)\n",
    "    random.shuffle(file_lst)\n",
    "    \n",
    "    train = []\n",
    "    valid = []\n",
    "    test = []\n",
    "    \n",
    "    for file in file_lst :\n",
    "        try : \n",
    "            emotion = int(file.split(\"-\")[2])\n",
    "            actor = int(file.split(\"-\")[6].split(\".\")[0])\n",
    "            spectrogram = scipy.io.loadmat(path+file)[\"S\"]\n",
    "        \n",
    "            if actor in [1,2] :\n",
    "                valid.append((spectrogram, emotion))\n",
    "            elif actor in [3,4] :\n",
    "                test.append((spectrogram, emotion))\n",
    "            else :\n",
    "                train.append((spectrogram, emotion))\n",
    "                \n",
    "        except :\n",
    "            pass\n",
    "    \n",
    "    return file_lst, train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav_data(path) :\n",
    "    file_lst = os.listdir(path)\n",
    "    random.shuffle(file_lst)\n",
    "    \n",
    "    train = []\n",
    "    valid = []\n",
    "    test = []\n",
    "    all_data = []\n",
    "    \n",
    "    for file in file_lst :\n",
    "        try : \n",
    "            y, sr = librosa.load(path+file)\n",
    "            emotion = int(file.split(\"-\")[2])\n",
    "            actor = int(file.split(\"-\")[6].split(\".\")[0])\n",
    "        \n",
    "            melspectrogram = librosa.feature.melspectrogram(y, sr=sr, n_mels=128)\n",
    "        \n",
    "            if actor in [1,2] :\n",
    "                valid.append((melspectrogram, emotion))\n",
    "            elif actor in [3,4] :\n",
    "                test.append((melspectrogram, emotion))\n",
    "            else :\n",
    "                train.append((melspectrogram, emotion))\n",
    "            \n",
    "            all_data.append((melspectrogram, emotion))\n",
    "        except :\n",
    "            pass\n",
    "    \n",
    "    return file_lst, train, valid, test, all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding(train, valid, test, num=528) :\n",
    "    result = []\n",
    "    \n",
    "    for dataset in [train, valid, test] :\n",
    "        zero = np.zeros([len(dataset), 1025, num])\n",
    "        emotion_lst = []\n",
    "\n",
    "        idx = 0\n",
    "        for spectrogram, emotion in dataset:\n",
    "            zero[idx, :, 0:len(spectrogram[0])] = spectrogram\n",
    "            emotion_lst.append(emotion-1)\n",
    "            idx += 1\n",
    "            \n",
    "        result.append((zero, emotion_lst))\n",
    "        \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutting(train, valid, test, all_data, size=1025, num=276) :\n",
    "    result = []\n",
    "    half = int(num/2)\n",
    "    \n",
    "    for dataset in [train, valid, test, all_data] :\n",
    "        zero = np.zeros([len(dataset), size, num])\n",
    "        emotion_lst = []\n",
    "\n",
    "        idx = 0\n",
    "        for spectrogram, emotion in dataset:\n",
    "            mid = int(spectrogram.shape[1]/2)\n",
    "            zero[idx, :, 0:len(spectrogram[0])] = spectrogram[:, mid-half:mid+half]\n",
    "            emotion_lst.append(emotion-1)\n",
    "            idx += 1\n",
    "            \n",
    "        result.append((zero, emotion_lst))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encoding(data, num=8) :\n",
    "    return np.eye(num)[data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectrogram from mat\n",
    "\"\"\"\n",
    "file_lst, train, valid, test = load_mat_data(\"data/mat/\")\n",
    "train, valid, test = cutting(train, valid, test)\n",
    "\n",
    "train_data = train[0].reshape([-1, 1025, 276, 1])\n",
    "train_label = onehot_encoding(train[1])\n",
    "\n",
    "valid_data = valid[0].reshape([-1, 1025, 276, 1])\n",
    "valid_label = onehot_encoding(valid[1])\n",
    "\n",
    "test_data = test[0].reshape([-1, 1025, 276, 1])\n",
    "test_label = onehot_encoding(test[1])\n",
    "\n",
    "train = []\n",
    "valid = []\n",
    "test = []\n",
    "\n",
    "print(len(train_data), train_data.shape, train_label.shape)\n",
    "print(len(valid_data))\n",
    "print(len(test_data))\n",
    "\n",
    "print(file_lst[:5])\n",
    "print(train_data[0])\n",
    "plt.imshow(train_data[300].reshape(1025,276))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melspectrogram from wav\n",
    "file_lst, train, valid, test, all_data = load_wav_data(\"data/wav/\")\n",
    "cut_train, cut_valid, cut_test, cut_all = cutting(train, valid, test, all_data, size =128 , num=126)\n",
    "\n",
    "train_data = cut_train[0].reshape([-1, 128, 126, 1])\n",
    "train_label = onehot_encoding(cut_train[1])\n",
    "\n",
    "valid_data = cut_valid[0].reshape([-1, 128, 126, 1])\n",
    "valid_label = onehot_encoding(cut_valid[1])\n",
    "\n",
    "test_data = cut_test[0].reshape([-1, 128, 126, 1])\n",
    "test_label = onehot_encoding(cut_test[1])\n",
    "\n",
    "all_data = cut_all[0].reshape([-1, 128, 126, 1])\n",
    "all_label = onehot_encoding(cut_all[1])\n",
    "\n",
    "train = []\n",
    "valid = []\n",
    "test = []\n",
    "cut_train = []\n",
    "cut_valid = []\n",
    "cut_test = []\n",
    "cut_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data), train_data.shape, train_label.shape)\n",
    "print(len(valid_data))\n",
    "print(len(test_data))\n",
    "print(train_data[0][:5, :5])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(librosa.power_to_db(train_data[0].reshape(128,126), ref=np.max),y_axis='mel', fmax=8000,x_axis='time')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel spectrogram')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <br></br>  <br></br>  <br></br>  <br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.02\n",
    "training_epochs = 100\n",
    "batch_size = 40\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "\n",
    "valid_losses = []\n",
    "valid_accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "model = CNN(sess, \"CNN\")\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Learning Started!')\n",
    "print(\"\")\n",
    "\n",
    "# train model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(len(train_data) / batch_size)\n",
    "    idx = 0\n",
    "    \n",
    "    learning_rate = 0.001\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = train_data[idx:idx+batch_size], train_label[idx:idx+batch_size]\n",
    "        c, _ = model.train(batch_xs, batch_ys, learning_rate=learning_rate)\n",
    "        avg_cost += c / total_batch\n",
    "        idx += batch_size\n",
    "        \n",
    "        if i%10 == 0 :\n",
    "            print(\"log : \", avg_cost)\n",
    "            \n",
    "    #train cost & acc\n",
    "    cost, acc = model.evaluate(train_data, train_label, batch_size = batch_size)\n",
    "    \n",
    "    train_losses.append(cost)\n",
    "    train_accs.append(acc)\n",
    "    \n",
    "    #valid cost & acc\n",
    "    v_cost, v_acc = model.evaluate(valid_data, valid_label, batch_size = batch_size)\n",
    "    \n",
    "    valid_losses.append(v_cost)\n",
    "    valid_accs.append(v_acc)\n",
    "    \n",
    "    print(\"epoch : \", epoch, \" -- train {:.5f}({:.1f}%), valid {:.5f}({:.1f}%)\".format(cost, acc*100, v_cost, v_acc*100))\n",
    "    print(\" \")\n",
    "\n",
    "print(\"\")\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy:', model.get_accuracy(test_data, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ver = 2\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, \"./CNN/CNN_model/ver_\" + str(ver) +\"/CNN_\" + str(ver) + \".ckpt\")\n",
    "print(\"model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <br></br>  <br></br>  <br></br>  <br></br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
