{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkling Emoticana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import tensorflowonspark as TFOS\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import nested_scopes\n",
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import math\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflowonspark import dfutil\n",
    "from tensorflowonspark import TFCluster\n",
    "from tensorflowonspark.pipeline import TFEstimator, TFModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br> <br></br> <br></br> <br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encoding(data, num=8) :\n",
    "    return np.eye(num)[data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutting(train, valid, test, size=1025, num=276) :\n",
    "    result = []\n",
    "    half = int(num/2)\n",
    "    \n",
    "    for dataset in [train, valid, test] :\n",
    "        if not dataset :\n",
    "            continue\n",
    "            \n",
    "        zero = np.zeros([len(dataset), size, num])\n",
    "        emotion_lst = []\n",
    "\n",
    "        idx = 0\n",
    "        for spectrogram, emotion in dataset:\n",
    "            mid = int(spectrogram.shape[1]/2)\n",
    "            zero[idx, :, 0:len(spectrogram[0])] = spectrogram[:, mid-half:mid+half]\n",
    "            emotion_lst.append(emotion-1)\n",
    "            idx += 1\n",
    "            \n",
    "        result.append((zero, emotion_lst))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav_data(path) :\n",
    "    file_lst = os.listdir(path)\n",
    "    random.shuffle(file_lst)\n",
    "    \n",
    "    train = []\n",
    "    valid = []\n",
    "    test = []\n",
    "    \n",
    "    for file in file_lst :\n",
    "        try : \n",
    "            y, sr = librosa.load(path+file)\n",
    "            emotion = int(file.split(\"-\")[2])\n",
    "            actor = int(file.split(\"-\")[6].split(\".\")[0])\n",
    "        \n",
    "            melspectrogram = librosa.feature.melspectrogram(y, sr=sr, n_mels=128)\n",
    "        \n",
    "            if actor in [1,2] :\n",
    "                valid.append((melspectrogram, emotion))\n",
    "            elif actor in [3,4] :\n",
    "                test.append((melspectrogram, emotion))\n",
    "            else :\n",
    "                train.append((melspectrogram, emotion))\n",
    "                \n",
    "        except :\n",
    "            pass\n",
    "    \n",
    "    return file_lst, train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav_test_data(path) :\n",
    "    file_lst = os.listdir(path)\n",
    "    random.shuffle(file_lst)\n",
    "    \n",
    "    test = []\n",
    "    \n",
    "    for file in file_lst :\n",
    "        try : \n",
    "            y, sr = librosa.load(path+file)\n",
    "            emotion = int(file.split(\"-\")[2])\n",
    "            actor = int(file.split(\"-\")[6].split(\".\")[0])\n",
    "        \n",
    "            melspectrogram = librosa.feature.melspectrogram(y, sr=sr, n_mels=128)\n",
    "            test.append((melspectrogram, emotion))\n",
    "\n",
    "                \n",
    "        except :\n",
    "            pass\n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_data(df):\n",
    "    # Convert from dict of named arrays to two numpy arrays of the proper type\n",
    "    train_data = np.array(list(df.select('image').toPandas()['image'])).reshape([-1, 128, 126, 1])\n",
    "    train_label = np.array(list(df.select('label').toPandas()['label'])).reshape([-1, 8])\n",
    "        \n",
    "    return (train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_log(worker_num, arg):\n",
    "    print(\"{0}: {1}\".format(worker_num, arg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_df(data, label) :\n",
    "    data_rdd = sc.parallelize(data.reshape([-1, 128, 126]).tolist())\n",
    "    label_rdd = sc.parallelize(label.reshape([-1, 8]).tolist())\n",
    "    \n",
    "    pair = data_rdd.zip(label_rdd)\n",
    "    df = spark.createDataFrame(pair, ['image', 'label'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br> <br></br> <br></br> <br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN() :\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def convolution(self, X_input, filters, kernel_size, strides, name, padding=\"SAME\") :\n",
    "        with tf.variable_scope(name) :\n",
    "            bn = tf.layers.batch_normalization(X_input)\n",
    "            conv = tf.layers.conv2d(bn, filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            relu = tf.nn.leaky_relu(conv)\n",
    "            \n",
    "            return relu\n",
    "            \n",
    "    def build(self) :\n",
    "        with tf.variable_scope(self.name) :\n",
    "            ### Input\n",
    "            #input : 128x126x1\n",
    "            #output : 8\n",
    "            self.X = tf.placeholder(tf.float32, [None, 128, 126, 1])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, 8])\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "            self.learning_rate = tf.placeholder(tf.float32)\n",
    "            print(self.X.shape)\n",
    "            \n",
    "        ### Input Layer\n",
    "        #input : 128x126x1\n",
    "        #output : 32x31x8\n",
    "        conv1 = self.convolution(self.X, 8, [3,3], 2, \"conv1\")\n",
    "        pool1 = tf.layers.max_pooling2d(conv1, pool_size=[2,2], strides=2, name=\"pool1\")\n",
    "        print(conv1.shape)\n",
    "        print(pool1.shape)\n",
    "\n",
    "        ### Hidden Layer1\n",
    "        #input : 32x31x8\n",
    "        #output : 32x31x16\n",
    "        conv2 = self.convolution(conv1, 16, [3,3], 1, \"conv2\")\n",
    "        print(conv2.shape)\n",
    "            \n",
    "        ### Hidden Layer2\n",
    "        #input : 32x31x16\n",
    "        #output : 32x31x32\n",
    "        conv3 = self.convolution(conv2, 32, [3,3], 1, \"conv3\")\n",
    "        print(conv3.shape)\n",
    "            \n",
    "        ### Pooling Layer2\n",
    "        #input : 32x31x32\n",
    "        #output : 16x15x32\n",
    "        pool2 = tf.layers.max_pooling2d(conv3, pool_size=[2,2], strides=2, name=\"pool2\")\n",
    "        print(pool2.shape)\n",
    "            \n",
    "        ### Hidden Layer3\n",
    "        #input : 16x15x32\n",
    "        #output : 16x15x64\n",
    "        conv4 = self.convolution(pool2, 64, [3,3], 1, \"conv4\")\n",
    "        print(conv4.shape)\n",
    "        \n",
    "        ### Hidden Layer4\n",
    "        #input : 16x15x64\n",
    "        #output : 16x15x128\n",
    "        conv5 = self.convolution(conv4, 128, [3,3], 1, \"conv5\")\n",
    "        print(conv5.shape)\n",
    "        \n",
    "        ### Pooling Layer3\n",
    "        #input : 16x15x128\n",
    "        #output : 8x7x128\n",
    "        pool3 = tf.layers.max_pooling2d(conv5, pool_size=[2,2], strides=2, name=\"pool3\")\n",
    "        print(pool3.shape)\n",
    "        \n",
    "        ### Hidden Layer5\n",
    "        #input : 8x7x128\n",
    "        #output : 8x7x32\n",
    "        conv6 = self.convolution(pool3, 32, [1,1], 1, \"conv6\")\n",
    "        print(conv6.shape)\n",
    "        \n",
    "        with tf.variable_scope(\"global_avg_pooling\") :\n",
    "            ### global avg pooling\n",
    "            #input : 8x7x32\n",
    "            #output : 1x1x32\n",
    "            global_avg_pooling = tf.reduce_mean(conv6, [1, 2], keep_dims=True)\n",
    "            print(global_avg_pooling.shape)\n",
    "        \n",
    "        with tf.variable_scope(\"fully_connected\") :\n",
    "            ###Output Layer\n",
    "            #input : 1x1x32\n",
    "            #ouput : 8\n",
    "            shape = global_avg_pooling.get_shape().as_list()\n",
    "            dimension = shape[1] * shape[2] * shape[3]\n",
    "            self.flat = tf.reshape(global_avg_pooling, shape=[-1, dimension])\n",
    "\n",
    "            fc = tf.layers.dense(inputs=self.flat, units=8, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.logits = fc\n",
    "\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.Y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "        \n",
    "        self.label = tf.argmax(self.logits, 1)\n",
    "        correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))     \n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "    def set_sess(self, sess) :\n",
    "        self.sess = sess\n",
    "\n",
    "    def predict(self, x_test, training=False):\n",
    "        feed_dict={self.X: x_test, self.training: training}\n",
    "        \n",
    "        return self.sess.run(self.logits, feed_dict=feed_dict)\n",
    "\n",
    "    def get_accuracy(self, x_test, y_test, training=False):\n",
    "        feed_dict={self.X: x_test,self.Y: y_test, self.training: training}\n",
    "        \n",
    "        return self.sess.run(self.accuracy, feed_dict=feed_dict)\n",
    "\n",
    "    def train(self, x_data, y_data, learning_rate, training=True):\n",
    "        feed_dict={self.X: x_data, self.Y: y_data, self.learning_rate: learning_rate, self.training: training}\n",
    "        \n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict=feed_dict)\n",
    "    \n",
    "    def evaluate(self, X_input, Y_input, batch_size=None, training=False):\n",
    "        N = X_input.shape[0]\n",
    "            \n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "            \n",
    "        for i in range(0, N, batch_size):\n",
    "            X_batch = X_input[i:i + batch_size]\n",
    "            Y_batch = Y_input[i:i + batch_size]\n",
    "                \n",
    "            feed_dict = {self.X: X_batch, self.Y: Y_batch, self.training: training}\n",
    "                \n",
    "            loss = self.cost\n",
    "            accuracy = self.accuracy\n",
    "                \n",
    "            step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed_dict)\n",
    "                \n",
    "            total_loss += step_loss * X_batch.shape[0]\n",
    "            total_acc += step_acc * X_batch.shape[0]\n",
    "            \n",
    "        total_loss /= N\n",
    "        total_acc /= N\n",
    "            \n",
    "        return total_loss, total_acc\n",
    "    \n",
    "    def save(self, ver) :\n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(self.sess, \"CNN_\" + str(ver) + \".ckpt\")\n",
    "        \n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br> <br></br> <br></br> <br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_executors = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--epochs\", help=\"number of epochs\", type=int, default=50)\n",
    "parser.add_argument(\"--data\", help=\"HDFS path to MNIST data in parallelized format\")\n",
    "parser.add_argument(\"--format\", help=\"format\", default=\"wav\")\n",
    "parser.add_argument(\"--model_dir\", help=\"HDFS path to save/load model during train/test\")\n",
    "parser.add_argument(\"--readers\", help=\"number of reader/enqueue threads\", type=int, default=1)\n",
    "parser.add_argument(\"--steps\", help=\"maximum number of steps\", type=int, default=1000)\n",
    "parser.add_argument(\"--batch_size\", help=\"number of examples per batch\", type=int, default=40)\n",
    "parser.add_argument(\"--mode\", help=\"train|inference\", default=\"train\")\n",
    "parser.add_argument(\"--rdma\", help=\"use rdma connection\", default=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br> <br></br> <br></br> <br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TFoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_function(args, ctx):\n",
    "    \n",
    "    # Delay PS nodes a bit, since workers seem to reserve GPUs more quickly/reliably (w/o conflict)\n",
    "    if job_name == \"ps\":\n",
    "        time.sleep((worker_num + 1) * 5)\n",
    "        \n",
    "    # Get TF cluster and server instances\n",
    "    cluster, server = TFNode.start_cluster_server(0, args.rdma)\n",
    "\n",
    "    worker_num = ctx.worker_num\n",
    "    job_name = ctx.job_name\n",
    "    task_index = ctx.task_index\n",
    "    \n",
    "    height = 128\n",
    "    width = 126\n",
    "    batch_size = args.batch_size\n",
    "    \n",
    "    if job_name == \"ps\":\n",
    "        server.join()\n",
    "    elif job_name == \"worker\":\n",
    "\n",
    "        # Assigns ops to the local worker by default.\n",
    "        with tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" % task_index, cluster=cluster)):\n",
    "            model = CNN(\"CNN\")\n",
    "            global_step = tf.Variable(0)\n",
    "            \n",
    "            logit = model.logits\n",
    "            loss = model.cost\n",
    "            optimizer = model.optimizer\n",
    "            accuracy = model.accuracy\n",
    "            \n",
    "            tf.summary.scalar(\"loss\", loss)\n",
    "            tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "            summary_op = tf.summary.merge_all()\n",
    "            initializer = tf.global_variables_initializer()\n",
    "\n",
    "        # Create a \"supervisor\", which oversees the training process and stores model state into HDFS\n",
    "        logdir = TFNode.hdfs_path(ctx, args.model_dir)\n",
    "        print(\"tensorflow model path: {0}\".format(logdir))\n",
    "        hooks = [tf.train.StopAtStepHook(last_step=100000)]\n",
    "        \n",
    "        summary_writer = tf.summary.FileWriter(logdir, graph=tf.get_default_graph())\n",
    "\n",
    "        with tf.train.MonitoredTrainingSession(master=server.target,\n",
    "                                             is_chief=(task_index == 0),\n",
    "                                             checkpoint_dir=logdir,\n",
    "                                             hooks=hooks) as mon_sess:\n",
    "\n",
    "            step = 0\n",
    "            tf_feed = ctx.get_data_feed(args.mode == \"train\")\n",
    "            while not mon_sess.should_stop() and not tf_feed.should_stop() and step < args.steps:\n",
    "                batch_xs, batch_ys = get_batch_data(tf_feed.next_batch(batch_size))\n",
    "                 \n",
    "                feed_dict1 = {mode.X: batch_xs, model.Y: batch_ys, model.learning_rate: 0.008, self.training:True}\n",
    "                feed_dict2 = {mode.X: batch_xs, model.Y: batch_ys, model.learning_rate: 0.008, self.training:False}\n",
    "                \n",
    "                if len(batch_xs) > 0:\n",
    "                    if args.mode == \"train\" :\n",
    "                        _, summary, step = mon_sess.run([optimizer, summary_op, global_step], feed_dict=feed_dict1)\n",
    "\n",
    "                        if (step % 20 == 0):\n",
    "                            print(\"{0} step: {1} accuracy: {2}\".format(datetime.now().isoformat(), step, sess.run(accuracy, feed_dict = feed_dict2)))\n",
    "\n",
    "                        if task_index == 0:\n",
    "                            summary_writer.add_summary(summary, step)\n",
    "                    \n",
    "                    else :\n",
    "                        labels, preds, acc = mon_sess.run([model.label, logit, accuracy], feed_dict=feed_dict2)\n",
    "                        \n",
    "                        results = [\"{0} Label: {1}, Prediction: {2}\".format(datetime.now().isoformat(), l, p) for l, p in zip(labels, preds)]\n",
    "                        tf_feed.batch_results(results)\n",
    "                        print(\"results: {0}, acc: {1}\".format(results, acc))\n",
    "\n",
    "            if mon_sess.should_stop() or step >= args.steps:\n",
    "                tf_feed.terminate()\n",
    "\n",
    "        # Ask for all the services to stop.\n",
    "        print(\"{0} stopping MonitoredTrainingSession\".format(datetime.now().isoformat()))\n",
    "\n",
    "    if job_name == \"worker\" and task_index == 0:\n",
    "        summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_lst, train, valid, test = load_wav_data(data_dir)\n",
    "cut_train, cut_valid, cut_test = cutting(train, valid, test, size =128 , num=126)\n",
    "\n",
    "train_data = cut_train[0].reshape([-1, 128, 126, 1])\n",
    "train_label = onehot_encoding(cut_train[1])\n",
    "    \n",
    "test_data = cut_test[0].reshape([-1, 128, 126, 1])\n",
    "test_label = onehot_encoding(cut_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(['--mode', 'train', '--steps', '3000', '--epochs', '5',\n",
    "                          '--data', data_dir])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = TFCluster.run(sc, CNN_function, args, num_executors, 1, False, TFCluster.InputMode.SPARK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = np_to_df(train_data.to, train_label)\n",
    "cluster.train(trainDF, args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(['--mode', 'inference', \n",
    "                          '--data', data_dir])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = TFCluster.run(sc, CNN_function, args, num_executors, 1, False, TFCluster.InputMode.SPARK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF = np_to_rdd(test_data, test_label)\n",
    "\n",
    "prediction_results = cluster.inference(testDF)\n",
    "prediction_results.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
