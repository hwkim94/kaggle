{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sparkling Emoticana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. denseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class denseNet1() :\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        \n",
    "    def dense_block(self, X_input, num_filter, num_layer, name) :\n",
    "        with tf.variable_scope(name) :\n",
    "            layer_lst = [X_input]\n",
    "            concat = X_input\n",
    "            \n",
    "            for idx in range(num_layer) :\n",
    "                bn1 = tf.layers.batch_normalization(inputs=concat)\n",
    "                relu1 = tf.nn.leaky_relu(bn1)\n",
    "                conv1 = tf.layers.conv2d(inputs=relu1, filters=num_filter*4, kernel_size=[1,1], strides=1, padding=\"SAME\", kernel_initializer=tf.contrib.layers.xavier_initializer())  \n",
    "                    \n",
    "                bn2 = tf.layers.batch_normalization(inputs=conv1)\n",
    "                relu2 = tf.nn.leaky_relu(bn2)\n",
    "                conv2 = tf.layers.conv2d(inputs=relu2, filters=num_filter, kernel_size=[3,3], strides=1, padding=\"SAME\", kernel_initializer=tf.contrib.layers.xavier_initializer())  \n",
    "\n",
    "                layer_lst.append(conv2)\n",
    "                concat = tf.concat(layer_lst, axis=3)\n",
    "                \n",
    "            return concat\n",
    "        \n",
    "    def transition_block(self, X_input, theta, name) :\n",
    "        with tf.variable_scope(name) :\n",
    "            depth = X_input.get_shape().as_list()[3]\n",
    "            \n",
    "            bn = tf.layers.batch_normalization(inputs=X_input)\n",
    "            relu = tf.nn.leaky_relu(bn)\n",
    "            conv = tf.layers.conv2d(inputs=relu, filters=depth*theta, kernel_size=[1,1], strides=1, padding=\"SAME\", kernel_initializer=tf.contrib.layers.xavier_initializer())  \n",
    "            pool = tf.layers.average_pooling2d(inputs=conv, pool_size=[2,2], strides=2,)     \n",
    "            \n",
    "            return pool\n",
    "        \n",
    "    def build(self, k_init=40, k=12) :\n",
    "        with tf.variable_scope(self.name) :\n",
    "            ### Input\n",
    "            #input : 128x126x1\n",
    "            #output : 8\n",
    "            self.X = tf.placeholder(tf.float32, [None, 128, 126, 1])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, 8])\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "            self.learning_rate = tf.placeholder(tf.float32)\n",
    "            print(self.X.shape)\n",
    "\n",
    "            ### Input Layer\n",
    "            #input : 128x126x1\n",
    "            #output : 64x63x40\n",
    "            self.bn = tf.layers.batch_normalization(self.X)\n",
    "            self.conv = tf.layers.conv2d(inputs=self.bn, filters=k_init, kernel_size=[3,3], strides=2, padding=\"SAME\", kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.relu = tf.nn.leaky_relu(self.conv)\n",
    "            \n",
    "            print(self.conv.shape)\n",
    "            print(\"-\"*10)\n",
    "            \n",
    "        ### Hidden Layer\n",
    "        #input : 64x63x40\n",
    "        #output : 32x31x84\n",
    "        self.dense1 = self.dense_block(self.relu, k, 8, \"dense1\")\n",
    "        self.transition1 = self.transition_block(self.dense1, 0.5, name=\"transition1\")\n",
    "        print(self.dense1.shape)\n",
    "        print(self.transition1.shape)\n",
    "        print(\"-\"*10)\n",
    "        \n",
    "        #input : 32x31x84\n",
    "        #output : 16x15x138\n",
    "        self.dense2 = self.dense_block(self.transition1, k, 12, \"dense2\")\n",
    "        self.transition2 = self.transition_block(self.dense2, 0.5, name=\"transition2\")\n",
    "        print(self.dense2.shape)\n",
    "        print(self.transition2.shape)\n",
    "        print(\"-\"*10)\n",
    "        \n",
    "            \n",
    "        #input : 16x15x138\n",
    "        #output : 8x7x165\n",
    "        self.dense3 = self.dense_block(self.transition2, k, 12, \"dense3\")\n",
    "        self.transition3 = self.transition_block(self.dense3, 0.5, name=\"transition3\")\n",
    "        print(self.dense3.shape)\n",
    "        print(self.transition3.shape)\n",
    "        print(\"-\"*10)\n",
    "        \n",
    "        #input : 8x7x317\n",
    "        #output : 8x7x165\n",
    "        self.dense4 = self.dense_block(self.transition3, k, 8, \"dense4\")\n",
    "        print(self.dense4.shape)\n",
    "        print(\"-\"*10)\n",
    "        \n",
    "        #input : 8x7x165\n",
    "        #output : 1x1x165\n",
    "        with tf.variable_scope(\"global_avg_pooling\") :\n",
    "            self.bn = tf.layers.batch_normalization(self.dense4)\n",
    "            self.relu = tf.nn.leaky_relu(self.bn)\n",
    "            self.global_avg_pooling = tf.reduce_mean(self.relu, [1, 2], keep_dims=True)\n",
    "            print(self.global_avg_pooling.shape)\n",
    "            print(\"-\"*10)\n",
    "        \n",
    "        with tf.variable_scope(\"fully_connected\") :\n",
    "            ###Output Layer\n",
    "            #input : 1x1x165\n",
    "            #ouput : 8\n",
    "            shape = self.global_avg_pooling.get_shape().as_list()\n",
    "            dimension = shape[1] * shape[2] * shape[3]\n",
    "            self.flat = tf.reshape(self.global_avg_pooling, shape=[-1, dimension])\n",
    "\n",
    "            self.fc = tf.layers.dense(inputs=self.flat, units=8, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.logits = self.fc\n",
    "\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.Y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))     \n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, x_test, training=False):\n",
    "        feed_dict={self.X: x_test, self.training: training}\n",
    "        \n",
    "        return self.sess.run(self.logits, feed_dict=feed_dict)\n",
    "\n",
    "    def get_accuracy(self, x_test, y_test, training=False):\n",
    "        feed_dict={self.X: x_test,self.Y: y_test, self.training: training}\n",
    "        \n",
    "        return self.sess.run(self.accuracy, feed_dict=feed_dict)\n",
    "\n",
    "    def train(self, x_data, y_data, learning_rate, training=True):\n",
    "        feed_dict={self.X: x_data, self.Y: y_data, self.learning_rate: learning_rate, self.training: training}\n",
    "        \n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict=feed_dict)\n",
    "    \n",
    "    def evaluate(self, X_input, Y_input, batch_size=None, training=False):\n",
    "        N = X_input.shape[0]\n",
    "            \n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "            \n",
    "        for i in range(0, N, batch_size):\n",
    "            X_batch = X_input[i:i + batch_size]\n",
    "            Y_batch = Y_input[i:i + batch_size]\n",
    "                \n",
    "            feed_dict = {self.X: X_batch, self.Y: Y_batch, self.training: training}\n",
    "                \n",
    "            loss = self.cost\n",
    "            accuracy = self.accuracy\n",
    "                \n",
    "            step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed_dict)\n",
    "                \n",
    "            total_loss += step_loss * X_batch.shape[0]\n",
    "            total_acc += step_acc * X_batch.shape[0]\n",
    "            \n",
    "        total_loss /= N\n",
    "        total_acc /= N\n",
    "            \n",
    "        return total_loss, total_acc\n",
    "    \n",
    "    def save(self) :\n",
    "        saver = tf.train.Saver(ver)\n",
    "        save_path = saver.save(self.sess, \"denseNet_\" + str(ver) + \".ckpt\")\n",
    "        \n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class denseNet2() :\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        \n",
    "    def dense_block(self, X_input, num_filter, num_layer, name) :\n",
    "        with tf.variable_scope(name) :\n",
    "            layer_lst = [X_input]\n",
    "            concat = X_input\n",
    "            \n",
    "            for idx in range(num_layer) :\n",
    "                bn1 = tf.layers.batch_normalization(inputs=concat)\n",
    "                relu1 = tf.nn.leaky_relu(bn1)\n",
    "                conv1 = tf.layers.conv2d(inputs=relu1, filters=num_filter*4, kernel_size=[1,1], strides=1, padding=\"SAME\", kernel_initializer=tf.contrib.layers.xavier_initializer())  \n",
    "                    \n",
    "                bn2 = tf.layers.batch_normalization(inputs=conv1)\n",
    "                relu2 = tf.nn.leaky_relu(bn2)\n",
    "                conv2 = tf.layers.conv2d(inputs=relu2, filters=num_filter, kernel_size=[1,3], strides=1, padding=\"SAME\", kernel_initializer=tf.contrib.layers.xavier_initializer())  \n",
    "\n",
    "                layer_lst.append(conv2)\n",
    "                concat = tf.concat(layer_lst, axis=3)\n",
    "                \n",
    "            return concat\n",
    "        \n",
    "    def transition_block(self, X_input, theta, name) :\n",
    "        with tf.variable_scope(name) :\n",
    "            depth = X_input.get_shape().as_list()[3]\n",
    "            \n",
    "            bn = tf.layers.batch_normalization(inputs=X_input)\n",
    "            relu = tf.nn.leaky_relu(bn)\n",
    "            conv = tf.layers.conv2d(inputs=relu, filters=depth*theta, kernel_size=[1,1], strides=1, padding=\"SAME\", kernel_initializer=tf.contrib.layers.xavier_initializer())  \n",
    "            pool = tf.layers.average_pooling2d(inputs=conv, pool_size=[1,2], strides=2)     \n",
    "            \n",
    "            return pool\n",
    "        \n",
    "    def build(self, k_init=40, k=16) :\n",
    "        with tf.variable_scope(self.name) :\n",
    "            ### Input\n",
    "            #input : 128x126x1\n",
    "            #output : 8\n",
    "            self.X = tf.placeholder(tf.float32, [None, 128, 126, 1])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, 8])\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "            self.learning_rate = tf.placeholder(tf.float32)\n",
    "            print(self.X.shape)\n",
    "\n",
    "            ### Input Layer\n",
    "            #input : 128x126x1\n",
    "            #output : 1x63x40\n",
    "            self.bn = tf.layers.batch_normalization(self.X)\n",
    "            self.conv = tf.layers.conv2d(inputs=self.bn, filters=k_init, kernel_size=[128,3], strides=2, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.relu = tf.nn.leaky_relu(self.conv)\n",
    "            \n",
    "            print(self.conv.shape)\n",
    "            print(\"-\"*10)\n",
    "            \n",
    "        ### Hidden Layer\n",
    "        #input : 1x63x40\n",
    "        #output : 1x31x84\n",
    "        self.dense1 = self.dense_block(self.relu, k, 8, \"dense1\")\n",
    "        self.transition1 = self.transition_block(self.dense1, 0.5, name=\"transition1\")\n",
    "        print(self.dense1.shape)\n",
    "        print(self.transition1.shape)\n",
    "        print(\"-\"*10)\n",
    "        \n",
    "        #input : 1x31x84\n",
    "        #output : 1x15x138\n",
    "        self.dense2 = self.dense_block(self.transition1, k, 12, \"dense2\")\n",
    "        self.transition2 = self.transition_block(self.dense2, 0.5, name=\"transition2\")\n",
    "        print(self.dense2.shape)\n",
    "        print(self.transition2.shape)\n",
    "        print(\"-\"*10)\n",
    "        \n",
    "            \n",
    "        #input : 1x15x138\n",
    "        #output : 1x7x165\n",
    "        self.dense3 = self.dense_block(self.transition2, k, 12, \"dense3\")\n",
    "        self.transition3 = self.transition_block(self.dense3, 0.5, name=\"transition3\")\n",
    "        print(self.dense3.shape)\n",
    "        print(self.transition3.shape)\n",
    "        print(\"-\"*10)\n",
    "        \n",
    "        #input : 1x7x317\n",
    "        #output : 1x7x165\n",
    "        self.dense4 = self.dense_block(self.transition3, k, 8, \"dense4\")\n",
    "        print(self.dense4.shape)\n",
    "        print(\"-\"*10)\n",
    "        \n",
    "        #input : 1x7x165\n",
    "        #output : 1x1x165\n",
    "        with tf.variable_scope(\"global_avg_pooling\") :\n",
    "            self.bn = tf.layers.batch_normalization(self.dense4)\n",
    "            self.relu = tf.nn.leaky_relu(self.bn)\n",
    "            self.global_avg_pooling = tf.reduce_mean(self.relu, [1, 2], keep_dims=True)\n",
    "            print(self.global_avg_pooling.shape)\n",
    "            print(\"-\"*10)\n",
    "        \n",
    "        with tf.variable_scope(\"fully_connected\") :\n",
    "            ###Output Layer\n",
    "            #input : 1x1x165\n",
    "            #ouput : 8\n",
    "            shape = self.global_avg_pooling.get_shape().as_list()\n",
    "            dimension = shape[1] * shape[2] * shape[3]\n",
    "            self.flat = tf.reshape(self.global_avg_pooling, shape=[-1, dimension])\n",
    "\n",
    "            self.fc = tf.layers.dense(inputs=self.flat, units=8, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.logits = self.fc\n",
    "\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.Y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))     \n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, x_test, training=False):\n",
    "        feed_dict={self.X: x_test, self.training: training}\n",
    "        \n",
    "        return self.sess.run(self.logits, feed_dict=feed_dict)\n",
    "\n",
    "    def get_accuracy(self, x_test, y_test, training=False):\n",
    "        feed_dict={self.X: x_test,self.Y: y_test, self.training: training}\n",
    "        \n",
    "        return self.sess.run(self.accuracy, feed_dict=feed_dict)\n",
    "\n",
    "    def train(self, x_data, y_data, learning_rate, training=True):\n",
    "        feed_dict={self.X: x_data, self.Y: y_data, self.learning_rate: learning_rate, self.training: training}\n",
    "        \n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict=feed_dict)\n",
    "    \n",
    "    def evaluate(self, X_input, Y_input, batch_size=None, training=False):\n",
    "        N = X_input.shape[0]\n",
    "            \n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "            \n",
    "        for i in range(0, N, batch_size):\n",
    "            X_batch = X_input[i:i + batch_size]\n",
    "            Y_batch = Y_input[i:i + batch_size]\n",
    "                \n",
    "            feed_dict = {self.X: X_batch, self.Y: Y_batch, self.training: training}\n",
    "                \n",
    "            loss = self.cost\n",
    "            accuracy = self.accuracy\n",
    "                \n",
    "            step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed_dict)\n",
    "                \n",
    "            total_loss += step_loss * X_batch.shape[0]\n",
    "            total_acc += step_acc * X_batch.shape[0]\n",
    "            \n",
    "        total_loss /= N\n",
    "        total_acc /= N\n",
    "            \n",
    "        return total_loss, total_acc\n",
    "    \n",
    "    def save(self) :\n",
    "        saver = tf.train.Saver(ver)\n",
    "        save_path = saver.save(self.sess, \"denseNet_\" + str(ver) + \".ckpt\")\n",
    "        \n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <br></br>  <br></br>  <br></br>  <br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mat_data(path) :\n",
    "    file_lst = os.listdir(path)\n",
    "    random.shuffle(file_lst)\n",
    "    \n",
    "    train = []\n",
    "    valid = []\n",
    "    test = []\n",
    "    \n",
    "    for file in file_lst :\n",
    "        try : \n",
    "            emotion = int(file.split(\"-\")[2])\n",
    "            actor = int(file.split(\"-\")[6].split(\".\")[0])\n",
    "            spectrogram = scipy.io.loadmat(path+file)[\"S\"]\n",
    "        \n",
    "            if actor in [1,2] :\n",
    "                valid.append((spectrogram, emotion))\n",
    "            elif actor in [3,4] :\n",
    "                test.append((spectrogram, emotion))\n",
    "            else :\n",
    "                train.append((spectrogram, emotion))\n",
    "                \n",
    "        except :\n",
    "            pass\n",
    "    \n",
    "    return file_lst, train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav_data(path) :\n",
    "    file_lst = os.listdir(path)\n",
    "    random.shuffle(file_lst)\n",
    "    \n",
    "    train = []\n",
    "    valid = []\n",
    "    test = []\n",
    "    \n",
    "    for file in file_lst :\n",
    "        try : \n",
    "            y, sr = librosa.load(path+file)\n",
    "            emotion = int(file.split(\"-\")[2])\n",
    "            actor = int(file.split(\"-\")[6].split(\".\")[0])\n",
    "        \n",
    "            melspectrogram = librosa.feature.melspectrogram(y, sr=sr, n_mels=128)\n",
    "        \n",
    "            if actor in [1,2] :\n",
    "                valid.append((melspectrogram, emotion))\n",
    "            elif actor in [3,4] :\n",
    "                test.append((melspectrogram, emotion))\n",
    "            else :\n",
    "                train.append((melspectrogram, emotion))\n",
    "                \n",
    "        except :\n",
    "            pass\n",
    "    \n",
    "    return file_lst, train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding(train, valid, test, num=528) :\n",
    "    result = []\n",
    "    \n",
    "    for dataset in [train, valid, test] :\n",
    "        zero = np.zeros([len(dataset), 1025, num])\n",
    "        emotion_lst = []\n",
    "\n",
    "        idx = 0\n",
    "        for spectrogram, emotion in dataset:\n",
    "            zero[idx, :, 0:len(spectrogram[0])] = spectrogram\n",
    "            emotion_lst.append(emotion-1)\n",
    "            idx += 1\n",
    "            \n",
    "        result.append((zero, emotion_lst))\n",
    "        \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutting(train, valid, test, size=1025, num=276) :\n",
    "    result = []\n",
    "    half = int(num/2)\n",
    "    \n",
    "    for dataset in [train, valid, test] :\n",
    "        zero = np.zeros([len(dataset), size, num])\n",
    "        emotion_lst = []\n",
    "\n",
    "        idx = 0\n",
    "        for spectrogram, emotion in dataset:\n",
    "            mid = int(spectrogram.shape[1]/2)\n",
    "            zero[idx, :, 0:len(spectrogram[0])] = spectrogram[:, mid-half:mid+half]\n",
    "            emotion_lst.append(emotion-1)\n",
    "            idx += 1\n",
    "            \n",
    "        result.append((zero, emotion_lst))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encoding(data, num=8) :\n",
    "    return np.eye(num)[data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectrogram from mat\n",
    "\"\"\"\n",
    "file_lst, train, valid, test = load_mat_data(\"data/mat/\")\n",
    "train, valid, test = cutting(train, valid, test)\n",
    "\n",
    "train_data = train[0].reshape([-1, 1025, 276, 1])\n",
    "train_label = onehot_encoding(train[1])\n",
    "\n",
    "valid_data = valid[0].reshape([-1, 1025, 276, 1])\n",
    "valid_label = onehot_encoding(valid[1])\n",
    "\n",
    "test_data = test[0].reshape([-1, 1025, 276, 1])\n",
    "test_label = onehot_encoding(test[1])\n",
    "\n",
    "train = []\n",
    "valid = []\n",
    "test = []\n",
    "\n",
    "print(len(train_data), train_data.shape, train_label.shape)\n",
    "print(len(valid_data))\n",
    "print(len(test_data))\n",
    "\n",
    "print(file_lst[:5])\n",
    "print(train_data[0])\n",
    "plt.imshow(train_data[300].reshape(1025,276))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melspectrogram from wav\n",
    "file_lst, train, valid, test = load_wav_data(\"data/wav/\")\n",
    "cut_train, cut_valid, cut_test = cutting(train, valid, test, size =128 , num=126)\n",
    "\n",
    "train_data = cut_train[0].reshape([-1, 128, 126, 1])\n",
    "train_label = onehot_encoding(cut_train[1])\n",
    "\n",
    "valid_data = cut_valid[0].reshape([-1, 128, 126, 1])\n",
    "valid_label = onehot_encoding(cut_valid[1])\n",
    "\n",
    "test_data = cut_test[0].reshape([-1, 128, 126, 1])\n",
    "test_label = onehot_encoding(cut_test[1])\n",
    "\n",
    "train = []\n",
    "valid = []\n",
    "test = []\n",
    "cut_train = []\n",
    "cut_valid = []\n",
    "cut_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data), train_data.shape, train_label.shape)\n",
    "print(len(valid_data))\n",
    "print(len(test_data))\n",
    "print(train_data[0][:5, :5])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(librosa.power_to_db(train_data[0].reshape(128,126), ref=np.max),y_axis='mel', fmax=8000,x_axis='time')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel spectrogram')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <br></br>  <br></br>  <br></br>  <br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 40\n",
    "batch_size = 30\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "\n",
    "valid_losses = []\n",
    "valid_accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "model = denseNet1(sess, \"denseNet1\")\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "print('Learning Started!')\n",
    "print(\"\")\n",
    "\n",
    "# train model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(len(train_data) / batch_size)\n",
    "    idx = 0\n",
    "    if epoch == 1 :\n",
    "        learning_rate = 0.005\n",
    "    elif epoch == 3 :\n",
    "        learning_rate = 0.003\n",
    "    elif epoch == 5 :\n",
    "        learning_rate = 0.002\n",
    "    elif epoch == 10 :\n",
    "        learning_rate = 0.001\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = train_data[idx:idx+batch_size], train_label[idx:idx+batch_size]\n",
    "        c, _ = model.train(batch_xs, batch_ys, learning_rate=learning_rate)\n",
    "        avg_cost += c / total_batch\n",
    "        idx += batch_size\n",
    "        \n",
    "        if i%5 == 0 :\n",
    "            print(\"log : \", avg_cost)\n",
    "            \n",
    "    #train cost & acc\n",
    "    cost, acc = model.evaluate(train_data, train_label, batch_size = batch_size)\n",
    "    \n",
    "    train_losses.append(cost)\n",
    "    train_accs.append(acc)\n",
    "    \n",
    "    #valid cost & acc\n",
    "    v_cost, v_acc = model.evaluate(valid_data, valid_label, batch_size = batch_size)\n",
    "    \n",
    "    valid_losses.append(v_cost)\n",
    "    valid_accs.append(v_acc)\n",
    "    \n",
    "    print(\"epoch : \", epoch, \" -- train {:.5f}({:.1f}%), valid {:.5f}({:.1f}%)\".format(cost, acc*100, v_cost, v_acc*100))\n",
    "    print(\" \")\n",
    "\n",
    "print(\"\")\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Accuracy:', model.get_accuracy(test_data, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <br></br>  <br></br>  <br></br>  <br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
